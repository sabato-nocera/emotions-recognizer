Dataset used: ../../datasets/full_dataset.csv 

   Temperature  Humidity  Sound  ...     Z2  Classification  Feedback
0           32        95      1  ... -15596             100     Happy
1           32        86      1  ... -15628             100     Happy
2           -1        -1      1  ... -15612             100     Happy
3           -1        -1     -1  ...     -1             100     Happy
4           32        75      1  ... -15720             100     Happy

[5 rows x 12 columns]

Objservations: 8560
Reshaping:  ((6848, 11), (6848, 4), (1712, 11), (1712, 4))  -> ((6848, 11, 1), (6848, 4), (1712, 11, 1), (1712, 4))

Layers:

{'name': 'conv1d_1', 'trainable': True, 'batch_input_shape': (None, 11, 1), 'dtype': 'float32', 'filters': 512, 'kernel_size': (1,), 'strides': (1,), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1,), 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'activation_1', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'} 

{'name': 'flatten_1', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'} 

{'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.4, 'noise_shape': None, 'seed': None} 

{'name': 'dense_46', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_47', 'trainable': True, 'dtype': 'float32', 'units': 100, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_48', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'activation_2', 'trainable': True, 'dtype': 'float32', 'activation': 'softmax'} 

Compile: loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']

Start computation...

Train on 5478 samples, validate on 1370 samples
Epoch 1/160
 - 5s - loss: 0.9023 - accuracy: 0.6393 - val_loss: 0.7825 - val_accuracy: 0.7036
Epoch 2/160
 - 5s - loss: 0.7718 - accuracy: 0.7037 - val_loss: 0.7456 - val_accuracy: 0.7161
Epoch 3/160
 - 5s - loss: 0.7378 - accuracy: 0.7189 - val_loss: 0.7124 - val_accuracy: 0.7234
Epoch 4/160
 - 5s - loss: 0.7075 - accuracy: 0.7344 - val_loss: 0.6870 - val_accuracy: 0.7453
Epoch 5/160
 - 5s - loss: 0.6790 - accuracy: 0.7441 - val_loss: 0.6717 - val_accuracy: 0.7460
Epoch 6/160
 - 5s - loss: 0.6558 - accuracy: 0.7552 - val_loss: 0.6514 - val_accuracy: 0.7577
Epoch 7/160
 - 5s - loss: 0.6357 - accuracy: 0.7643 - val_loss: 0.6302 - val_accuracy: 0.7672
Epoch 8/160
 - 5s - loss: 0.6184 - accuracy: 0.7698 - val_loss: 0.6148 - val_accuracy: 0.7708
Epoch 9/160
 - 5s - loss: 0.5984 - accuracy: 0.7738 - val_loss: 0.5990 - val_accuracy: 0.7796
Epoch 10/160
 - 5s - loss: 0.5840 - accuracy: 0.7864 - val_loss: 0.5859 - val_accuracy: 0.7730
Epoch 11/160
 - 5s - loss: 0.5678 - accuracy: 0.7886 - val_loss: 0.5759 - val_accuracy: 0.7869
Epoch 12/160
 - 5s - loss: 0.5602 - accuracy: 0.7881 - val_loss: 0.5695 - val_accuracy: 0.7876
Epoch 13/160
 - 5s - loss: 0.5529 - accuracy: 0.7912 - val_loss: 0.5712 - val_accuracy: 0.7891
Epoch 14/160
 - 5s - loss: 0.5419 - accuracy: 0.7939 - val_loss: 0.5586 - val_accuracy: 0.7891
Epoch 15/160
 - 5s - loss: 0.5368 - accuracy: 0.7963 - val_loss: 0.5529 - val_accuracy: 0.7949
Epoch 16/160
 - 5s - loss: 0.5250 - accuracy: 0.7997 - val_loss: 0.5635 - val_accuracy: 0.7942
Epoch 17/160
 - 5s - loss: 0.5291 - accuracy: 0.7999 - val_loss: 0.5471 - val_accuracy: 0.8022
Epoch 18/160
 - 5s - loss: 0.5121 - accuracy: 0.8014 - val_loss: 0.5414 - val_accuracy: 0.7905
Epoch 19/160
 - 5s - loss: 0.5068 - accuracy: 0.8054 - val_loss: 0.5342 - val_accuracy: 0.8029
Epoch 20/160
 - 5s - loss: 0.5050 - accuracy: 0.8060 - val_loss: 0.5434 - val_accuracy: 0.7956
Epoch 21/160
 - 5s - loss: 0.4991 - accuracy: 0.8052 - val_loss: 0.5336 - val_accuracy: 0.8073
Epoch 22/160
 - 5s - loss: 0.4929 - accuracy: 0.8107 - val_loss: 0.5333 - val_accuracy: 0.7993
Epoch 23/160
 - 5s - loss: 0.4907 - accuracy: 0.8107 - val_loss: 0.5383 - val_accuracy: 0.7949
Epoch 24/160
 - 5s - loss: 0.4819 - accuracy: 0.8089 - val_loss: 0.5312 - val_accuracy: 0.8036
Epoch 25/160
 - 5s - loss: 0.4854 - accuracy: 0.8120 - val_loss: 0.5326 - val_accuracy: 0.8007
Epoch 26/160
 - 5s - loss: 0.4780 - accuracy: 0.8129 - val_loss: 0.5162 - val_accuracy: 0.8124
Epoch 27/160
 - 5s - loss: 0.4712 - accuracy: 0.8191 - val_loss: 0.5210 - val_accuracy: 0.8088
Epoch 28/160
 - 5s - loss: 0.4665 - accuracy: 0.8173 - val_loss: 0.5236 - val_accuracy: 0.8080
Epoch 29/160
 - 5s - loss: 0.4664 - accuracy: 0.8189 - val_loss: 0.5130 - val_accuracy: 0.8095
Epoch 30/160
 - 5s - loss: 0.4591 - accuracy: 0.8211 - val_loss: 0.5087 - val_accuracy: 0.8131
Epoch 31/160
 - 5s - loss: 0.4572 - accuracy: 0.8200 - val_loss: 0.5199 - val_accuracy: 0.8036
Epoch 32/160
 - 5s - loss: 0.4559 - accuracy: 0.8229 - val_loss: 0.5196 - val_accuracy: 0.8073
Epoch 33/160
 - 5s - loss: 0.4552 - accuracy: 0.8222 - val_loss: 0.5353 - val_accuracy: 0.8022
Epoch 34/160
 - 5s - loss: 0.4522 - accuracy: 0.8244 - val_loss: 0.5073 - val_accuracy: 0.8073
Epoch 35/160
 - 5s - loss: 0.4413 - accuracy: 0.8284 - val_loss: 0.5039 - val_accuracy: 0.8102
Epoch 36/160
 - 5s - loss: 0.4498 - accuracy: 0.8202 - val_loss: 0.5205 - val_accuracy: 0.8066
Epoch 37/160
 - 5s - loss: 0.4384 - accuracy: 0.8279 - val_loss: 0.5266 - val_accuracy: 0.8007
Epoch 38/160
 - 5s - loss: 0.4379 - accuracy: 0.8286 - val_loss: 0.5092 - val_accuracy: 0.8146
Epoch 39/160
 - 5s - loss: 0.4405 - accuracy: 0.8264 - val_loss: 0.4945 - val_accuracy: 0.8212
Epoch 40/160
 - 5s - loss: 0.4339 - accuracy: 0.8326 - val_loss: 0.5124 - val_accuracy: 0.8190
Epoch 41/160
 - 5s - loss: 0.4322 - accuracy: 0.8299 - val_loss: 0.5080 - val_accuracy: 0.8095
Epoch 42/160
 - 5s - loss: 0.4325 - accuracy: 0.8282 - val_loss: 0.4990 - val_accuracy: 0.8175
Epoch 43/160
 - 5s - loss: 0.4244 - accuracy: 0.8341 - val_loss: 0.4944 - val_accuracy: 0.8197
Epoch 44/160
 - 5s - loss: 0.4284 - accuracy: 0.8288 - val_loss: 0.5044 - val_accuracy: 0.8234
Epoch 45/160
 - 5s - loss: 0.4205 - accuracy: 0.8328 - val_loss: 0.4953 - val_accuracy: 0.8204
Epoch 46/160
 - 5s - loss: 0.4209 - accuracy: 0.8330 - val_loss: 0.5172 - val_accuracy: 0.8190
Epoch 47/160
 - 5s - loss: 0.4254 - accuracy: 0.8317 - val_loss: 0.5023 - val_accuracy: 0.8248
Epoch 48/160
 - 5s - loss: 0.4147 - accuracy: 0.8363 - val_loss: 0.4851 - val_accuracy: 0.8307
Epoch 49/160
 - 5s - loss: 0.4199 - accuracy: 0.8333 - val_loss: 0.4822 - val_accuracy: 0.8307
Epoch 50/160
 - 5s - loss: 0.4166 - accuracy: 0.8306 - val_loss: 0.5016 - val_accuracy: 0.8270
Epoch 51/160
 - 5s - loss: 0.4199 - accuracy: 0.8337 - val_loss: 0.4842 - val_accuracy: 0.8248
Epoch 52/160
 - 5s - loss: 0.4135 - accuracy: 0.8330 - val_loss: 0.4907 - val_accuracy: 0.8270
Epoch 53/160
 - 5s - loss: 0.4118 - accuracy: 0.8368 - val_loss: 0.4782 - val_accuracy: 0.8307
Epoch 54/160
 - 5s - loss: 0.4132 - accuracy: 0.8381 - val_loss: 0.4956 - val_accuracy: 0.8270
Epoch 55/160
 - 5s - loss: 0.4056 - accuracy: 0.8403 - val_loss: 0.4791 - val_accuracy: 0.8343
Epoch 56/160
 - 5s - loss: 0.4039 - accuracy: 0.8397 - val_loss: 0.4799 - val_accuracy: 0.8358
Epoch 57/160
 - 5s - loss: 0.4074 - accuracy: 0.8394 - val_loss: 0.4777 - val_accuracy: 0.8380
Epoch 58/160
 - 5s - loss: 0.4057 - accuracy: 0.8388 - val_loss: 0.4749 - val_accuracy: 0.8285
Epoch 59/160
 - 5s - loss: 0.4054 - accuracy: 0.8342 - val_loss: 0.4698 - val_accuracy: 0.8343
Epoch 60/160
 - 5s - loss: 0.4032 - accuracy: 0.8381 - val_loss: 0.4856 - val_accuracy: 0.8314
Epoch 61/160
 - 5s - loss: 0.4012 - accuracy: 0.8386 - val_loss: 0.5113 - val_accuracy: 0.8175
Epoch 62/160
 - 5s - loss: 0.4009 - accuracy: 0.8399 - val_loss: 0.4719 - val_accuracy: 0.8358
Epoch 63/160
 - 5s - loss: 0.3956 - accuracy: 0.8439 - val_loss: 0.4830 - val_accuracy: 0.8336
Epoch 64/160
 - 5s - loss: 0.3959 - accuracy: 0.8406 - val_loss: 0.4914 - val_accuracy: 0.8321
Epoch 65/160
 - 5s - loss: 0.3900 - accuracy: 0.8410 - val_loss: 0.4757 - val_accuracy: 0.8321
Epoch 66/160
 - 5s - loss: 0.3946 - accuracy: 0.8414 - val_loss: 0.4839 - val_accuracy: 0.8307
Epoch 67/160
 - 5s - loss: 0.3946 - accuracy: 0.8434 - val_loss: 0.4649 - val_accuracy: 0.8416
Epoch 68/160
 - 5s - loss: 0.3892 - accuracy: 0.8481 - val_loss: 0.4741 - val_accuracy: 0.8401
Epoch 69/160
 - 5s - loss: 0.3890 - accuracy: 0.8430 - val_loss: 0.4792 - val_accuracy: 0.8358
Epoch 70/160
 - 5s - loss: 0.3882 - accuracy: 0.8439 - val_loss: 0.4791 - val_accuracy: 0.8365
Epoch 71/160
 - 5s - loss: 0.3826 - accuracy: 0.8456 - val_loss: 0.4782 - val_accuracy: 0.8394
Epoch 72/160
 - 5s - loss: 0.3849 - accuracy: 0.8430 - val_loss: 0.4679 - val_accuracy: 0.8409
Epoch 73/160
 - 5s - loss: 0.3889 - accuracy: 0.8448 - val_loss: 0.4762 - val_accuracy: 0.8409
Epoch 74/160
 - 5s - loss: 0.3837 - accuracy: 0.8463 - val_loss: 0.4687 - val_accuracy: 0.8431
Epoch 75/160
 - 5s - loss: 0.3882 - accuracy: 0.8403 - val_loss: 0.4656 - val_accuracy: 0.8401
Epoch 76/160
 - 5s - loss: 0.3846 - accuracy: 0.8425 - val_loss: 0.4677 - val_accuracy: 0.8562
Epoch 77/160
 - 5s - loss: 0.3821 - accuracy: 0.8437 - val_loss: 0.4726 - val_accuracy: 0.8372
Epoch 78/160
 - 5s - loss: 0.3784 - accuracy: 0.8474 - val_loss: 0.4710 - val_accuracy: 0.8445
Epoch 79/160
 - 5s - loss: 0.3752 - accuracy: 0.8496 - val_loss: 0.4838 - val_accuracy: 0.8401
Epoch 80/160
 - 5s - loss: 0.3815 - accuracy: 0.8434 - val_loss: 0.4669 - val_accuracy: 0.8343
Epoch 81/160
 - 5s - loss: 0.3807 - accuracy: 0.8472 - val_loss: 0.4667 - val_accuracy: 0.8438
Epoch 82/160
 - 5s - loss: 0.3762 - accuracy: 0.8470 - val_loss: 0.4803 - val_accuracy: 0.8328
Epoch 83/160
 - 5s - loss: 0.3809 - accuracy: 0.8474 - val_loss: 0.4597 - val_accuracy: 0.8467
Epoch 84/160
 - 5s - loss: 0.3758 - accuracy: 0.8467 - val_loss: 0.4678 - val_accuracy: 0.8445
Epoch 85/160
 - 5s - loss: 0.3803 - accuracy: 0.8463 - val_loss: 0.4708 - val_accuracy: 0.8387
Epoch 86/160
 - 5s - loss: 0.3722 - accuracy: 0.8521 - val_loss: 0.4769 - val_accuracy: 0.8409
Epoch 87/160
 - 5s - loss: 0.3708 - accuracy: 0.8492 - val_loss: 0.4636 - val_accuracy: 0.8504
Epoch 88/160
 - 5s - loss: 0.3707 - accuracy: 0.8503 - val_loss: 0.4642 - val_accuracy: 0.8467
Epoch 89/160
 - 5s - loss: 0.3698 - accuracy: 0.8463 - val_loss: 0.4584 - val_accuracy: 0.8460
Epoch 90/160
 - 5s - loss: 0.3705 - accuracy: 0.8496 - val_loss: 0.4621 - val_accuracy: 0.8438
Epoch 91/160
 - 5s - loss: 0.3660 - accuracy: 0.8520 - val_loss: 0.4728 - val_accuracy: 0.8445
Epoch 92/160
 - 5s - loss: 0.3696 - accuracy: 0.8516 - val_loss: 0.4672 - val_accuracy: 0.8453
Epoch 93/160
 - 5s - loss: 0.3728 - accuracy: 0.8476 - val_loss: 0.4565 - val_accuracy: 0.8467
Epoch 94/160
 - 5s - loss: 0.3658 - accuracy: 0.8509 - val_loss: 0.4750 - val_accuracy: 0.8438
Epoch 95/160
 - 5s - loss: 0.3683 - accuracy: 0.8470 - val_loss: 0.4556 - val_accuracy: 0.8511
Epoch 96/160
 - 5s - loss: 0.3660 - accuracy: 0.8488 - val_loss: 0.4621 - val_accuracy: 0.8511
Epoch 97/160
 - 5s - loss: 0.3647 - accuracy: 0.8509 - val_loss: 0.4590 - val_accuracy: 0.8489
Epoch 98/160
 - 5s - loss: 0.3654 - accuracy: 0.8518 - val_loss: 0.4598 - val_accuracy: 0.8474
Epoch 99/160
 - 5s - loss: 0.3625 - accuracy: 0.8523 - val_loss: 0.4563 - val_accuracy: 0.8431
Epoch 100/160
 - 5s - loss: 0.3671 - accuracy: 0.8509 - val_loss: 0.4527 - val_accuracy: 0.8445
Epoch 101/160
 - 5s - loss: 0.3653 - accuracy: 0.8512 - val_loss: 0.4676 - val_accuracy: 0.8482
Epoch 102/160
 - 5s - loss: 0.3578 - accuracy: 0.8520 - val_loss: 0.4707 - val_accuracy: 0.8482
Epoch 103/160
 - 5s - loss: 0.3603 - accuracy: 0.8525 - val_loss: 0.4727 - val_accuracy: 0.8489
Epoch 104/160
 - 5s - loss: 0.3589 - accuracy: 0.8545 - val_loss: 0.4528 - val_accuracy: 0.8453
Epoch 105/160
 - 5s - loss: 0.3636 - accuracy: 0.8518 - val_loss: 0.4466 - val_accuracy: 0.8504
Epoch 106/160
 - 5s - loss: 0.3515 - accuracy: 0.8540 - val_loss: 0.4568 - val_accuracy: 0.8504
Epoch 107/160
 - 5s - loss: 0.3649 - accuracy: 0.8467 - val_loss: 0.4743 - val_accuracy: 0.8438
Epoch 108/160
 - 5s - loss: 0.3567 - accuracy: 0.8530 - val_loss: 0.4597 - val_accuracy: 0.8423
Epoch 109/160
 - 5s - loss: 0.3589 - accuracy: 0.8532 - val_loss: 0.4622 - val_accuracy: 0.8423
Epoch 110/160
 - 5s - loss: 0.3587 - accuracy: 0.8529 - val_loss: 0.4628 - val_accuracy: 0.8423
Epoch 111/160
 - 5s - loss: 0.3513 - accuracy: 0.8523 - val_loss: 0.4586 - val_accuracy: 0.8438
Epoch 112/160
 - 5s - loss: 0.3563 - accuracy: 0.8514 - val_loss: 0.4693 - val_accuracy: 0.8431
Epoch 113/160
 - 5s - loss: 0.3576 - accuracy: 0.8529 - val_loss: 0.4624 - val_accuracy: 0.8445
Epoch 114/160
 - 5s - loss: 0.3620 - accuracy: 0.8545 - val_loss: 0.4559 - val_accuracy: 0.8482
Epoch 115/160
 - 5s - loss: 0.3503 - accuracy: 0.8520 - val_loss: 0.4570 - val_accuracy: 0.8423
Epoch 116/160
 - 5s - loss: 0.3510 - accuracy: 0.8536 - val_loss: 0.4542 - val_accuracy: 0.8467
Epoch 117/160
 - 5s - loss: 0.3532 - accuracy: 0.8536 - val_loss: 0.4546 - val_accuracy: 0.8453
Epoch 118/160
 - 5s - loss: 0.3537 - accuracy: 0.8554 - val_loss: 0.4513 - val_accuracy: 0.8526
Epoch 119/160
 - 5s - loss: 0.3583 - accuracy: 0.8490 - val_loss: 0.4722 - val_accuracy: 0.8511
Epoch 120/160
 - 5s - loss: 0.3515 - accuracy: 0.8563 - val_loss: 0.4647 - val_accuracy: 0.8496
Epoch 121/160
 - 5s - loss: 0.3517 - accuracy: 0.8532 - val_loss: 0.4576 - val_accuracy: 0.8445
Epoch 122/160
 - 5s - loss: 0.3526 - accuracy: 0.8541 - val_loss: 0.4584 - val_accuracy: 0.8511
Epoch 123/160
 - 5s - loss: 0.3473 - accuracy: 0.8552 - val_loss: 0.4411 - val_accuracy: 0.8562
Epoch 124/160
 - 5s - loss: 0.3542 - accuracy: 0.8520 - val_loss: 0.4556 - val_accuracy: 0.8482
Epoch 125/160
 - 5s - loss: 0.3507 - accuracy: 0.8543 - val_loss: 0.4470 - val_accuracy: 0.8416
Epoch 126/160
 - 5s - loss: 0.3476 - accuracy: 0.8558 - val_loss: 0.4471 - val_accuracy: 0.8504
Epoch 127/160
 - 5s - loss: 0.3485 - accuracy: 0.8507 - val_loss: 0.4471 - val_accuracy: 0.8496
Epoch 128/160
 - 5s - loss: 0.3499 - accuracy: 0.8536 - val_loss: 0.4425 - val_accuracy: 0.8577
Epoch 129/160
 - 5s - loss: 0.3489 - accuracy: 0.8540 - val_loss: 0.4351 - val_accuracy: 0.8482
Epoch 130/160
 - 5s - loss: 0.3515 - accuracy: 0.8563 - val_loss: 0.4528 - val_accuracy: 0.8540
Epoch 131/160
 - 5s - loss: 0.3467 - accuracy: 0.8545 - val_loss: 0.4562 - val_accuracy: 0.8518
Epoch 132/160
 - 5s - loss: 0.3444 - accuracy: 0.8554 - val_loss: 0.4525 - val_accuracy: 0.8460
Epoch 133/160
 - 5s - loss: 0.3492 - accuracy: 0.8556 - val_loss: 0.4371 - val_accuracy: 0.8511
Epoch 134/160
 - 5s - loss: 0.3396 - accuracy: 0.8596 - val_loss: 0.4415 - val_accuracy: 0.8526
Epoch 135/160
 - 5s - loss: 0.3467 - accuracy: 0.8552 - val_loss: 0.4474 - val_accuracy: 0.8518
Epoch 136/160
 - 5s - loss: 0.3470 - accuracy: 0.8541 - val_loss: 0.4611 - val_accuracy: 0.8482
Epoch 137/160
 - 5s - loss: 0.3453 - accuracy: 0.8567 - val_loss: 0.4667 - val_accuracy: 0.8445
Epoch 138/160
 - 5s - loss: 0.3422 - accuracy: 0.8556 - val_loss: 0.4527 - val_accuracy: 0.8533
Epoch 139/160
 - 5s - loss: 0.3405 - accuracy: 0.8540 - val_loss: 0.4585 - val_accuracy: 0.8606
Epoch 140/160
 - 5s - loss: 0.3499 - accuracy: 0.8529 - val_loss: 0.4496 - val_accuracy: 0.8482
Epoch 141/160
 - 5s - loss: 0.3443 - accuracy: 0.8583 - val_loss: 0.4481 - val_accuracy: 0.8489
Epoch 142/160
 - 5s - loss: 0.3445 - accuracy: 0.8583 - val_loss: 0.4499 - val_accuracy: 0.8591
Epoch 143/160
 - 5s - loss: 0.3428 - accuracy: 0.8549 - val_loss: 0.4483 - val_accuracy: 0.8533
Epoch 144/160
 - 5s - loss: 0.3430 - accuracy: 0.8562 - val_loss: 0.4351 - val_accuracy: 0.8562
Epoch 145/160
 - 5s - loss: 0.3419 - accuracy: 0.8554 - val_loss: 0.4449 - val_accuracy: 0.8496
Epoch 146/160
 - 5s - loss: 0.3394 - accuracy: 0.8598 - val_loss: 0.4384 - val_accuracy: 0.8584
Epoch 147/160
 - 5s - loss: 0.3465 - accuracy: 0.8565 - val_loss: 0.4386 - val_accuracy: 0.8467
Epoch 148/160
 - 5s - loss: 0.3409 - accuracy: 0.8596 - val_loss: 0.4492 - val_accuracy: 0.8526
Epoch 149/160
 - 5s - loss: 0.3404 - accuracy: 0.8576 - val_loss: 0.4532 - val_accuracy: 0.8511
Epoch 150/160
 - 5s - loss: 0.3401 - accuracy: 0.8589 - val_loss: 0.4434 - val_accuracy: 0.8547
Epoch 151/160
 - 5s - loss: 0.3428 - accuracy: 0.8572 - val_loss: 0.4493 - val_accuracy: 0.8562
Epoch 152/160
 - 5s - loss: 0.3421 - accuracy: 0.8521 - val_loss: 0.4356 - val_accuracy: 0.8526
Epoch 153/160
 - 5s - loss: 0.3410 - accuracy: 0.8591 - val_loss: 0.4517 - val_accuracy: 0.8467
Epoch 154/160
 - 5s - loss: 0.3402 - accuracy: 0.8556 - val_loss: 0.4678 - val_accuracy: 0.8526
Epoch 155/160
 - 5s - loss: 0.3367 - accuracy: 0.8574 - val_loss: 0.4448 - val_accuracy: 0.8547
Epoch 156/160
 - 5s - loss: 0.3381 - accuracy: 0.8587 - val_loss: 0.4473 - val_accuracy: 0.8526
Epoch 157/160
 - 5s - loss: 0.3351 - accuracy: 0.8582 - val_loss: 0.4368 - val_accuracy: 0.8526
Epoch 158/160
 - 5s - loss: 0.3332 - accuracy: 0.8614 - val_loss: 0.4386 - val_accuracy: 0.8526
Epoch 159/160
 - 5s - loss: 0.3389 - accuracy: 0.8572 - val_loss: 0.4423 - val_accuracy: 0.8518
Epoch 160/160
 - 5s - loss: 0.3337 - accuracy: 0.8594 - val_loss: 0.4430 - val_accuracy: 0.8577

Fit: epochs = 160, batch_size = 27, verbose = 2, shuffle=False, validation_split = 0.20

Model: "sequential_10"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d_1 (Conv1D)            (None, 11, 512)           1024      
_________________________________________________________________
activation_1 (Activation)    (None, 11, 512)           0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5632)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 5632)              0         
_________________________________________________________________
dense_46 (Dense)             (None, 200)               1126600   
_________________________________________________________________
dense_47 (Dense)             (None, 100)               20100     
_________________________________________________________________
dense_48 (Dense)             (None, 4)                 404       
_________________________________________________________________
activation_2 (Activation)    (None, 4)                 0         
=================================================================
Total params: 1,148,128
Trainable params: 1,148,128
Non-trainable params: 0
_________________________________________________________________
None

Accuracy Train: 86.99%
Accuracy Test: 84.81%
Loss Train: 0.33
Loss Test: 0.43
Numero dati esaminati: 1712
True Positive 1452
False Positive 260
