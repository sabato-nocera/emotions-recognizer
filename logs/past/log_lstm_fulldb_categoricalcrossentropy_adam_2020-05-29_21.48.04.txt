Dataset used: ../../datasets/full_dataset.csv 

   Temperature  Humidity  Sound  ...     Z2  Classification  Feedback
0           32        95      1  ... -15596             100     Happy
1           32        86      1  ... -15628             100     Happy
2           -1        -1      1  ... -15612             100     Happy
3           -1        -1     -1  ...     -1             100     Happy
4           32        75      1  ... -15720             100     Happy

[5 rows x 12 columns]

Objservations: 8560
Reshaping:  ((6848, 11), (6848, 4), (1712, 11), (1712, 4))  -> ((6848, 1, 11), (6848, 4), (1712, 1, 11), (1712, 4))

Layers:

{'name': 'lstm_1', 'trainable': True, 'batch_input_shape': (None, 1, 11), 'dtype': 'float32', 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'units': 256, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 2} 

{'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 100, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

Compile: loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']

Start computation...

Train on 5478 samples, validate on 1370 samples
Epoch 1/7500
 - 1s - loss: 1.0878 - accuracy: 0.5562 - val_loss: 0.8837 - val_accuracy: 0.6635
Epoch 2/7500
 - 1s - loss: 0.8439 - accuracy: 0.6869 - val_loss: 0.8097 - val_accuracy: 0.6978
Epoch 3/7500
 - 1s - loss: 0.7964 - accuracy: 0.7023 - val_loss: 0.7809 - val_accuracy: 0.7109
Epoch 4/7500
 - 1s - loss: 0.7713 - accuracy: 0.7068 - val_loss: 0.7611 - val_accuracy: 0.7182
Epoch 5/7500
 - 1s - loss: 0.7517 - accuracy: 0.7112 - val_loss: 0.7448 - val_accuracy: 0.7255
Epoch 6/7500
 - 1s - loss: 0.7340 - accuracy: 0.7154 - val_loss: 0.7294 - val_accuracy: 0.7255
Epoch 7/7500
 - 1s - loss: 0.7177 - accuracy: 0.7256 - val_loss: 0.7162 - val_accuracy: 0.7270
Epoch 8/7500
 - 1s - loss: 0.7027 - accuracy: 0.7338 - val_loss: 0.7041 - val_accuracy: 0.7314
Epoch 9/7500
 - 1s - loss: 0.6893 - accuracy: 0.7424 - val_loss: 0.6926 - val_accuracy: 0.7336
Epoch 10/7500
 - 1s - loss: 0.6773 - accuracy: 0.7488 - val_loss: 0.6826 - val_accuracy: 0.7423
Epoch 11/7500
 - 1s - loss: 0.6658 - accuracy: 0.7530 - val_loss: 0.6712 - val_accuracy: 0.7474
Epoch 12/7500
 - 1s - loss: 0.6553 - accuracy: 0.7572 - val_loss: 0.6619 - val_accuracy: 0.7526
Epoch 13/7500
 - 1s - loss: 0.6444 - accuracy: 0.7618 - val_loss: 0.6528 - val_accuracy: 0.7547
Epoch 14/7500
 - 1s - loss: 0.6343 - accuracy: 0.7652 - val_loss: 0.6449 - val_accuracy: 0.7540
Epoch 15/7500
 - 1s - loss: 0.6250 - accuracy: 0.7665 - val_loss: 0.6373 - val_accuracy: 0.7562
Epoch 16/7500
 - 1s - loss: 0.6160 - accuracy: 0.7700 - val_loss: 0.6298 - val_accuracy: 0.7613
Epoch 17/7500
 - 1s - loss: 0.6074 - accuracy: 0.7729 - val_loss: 0.6231 - val_accuracy: 0.7650
Epoch 18/7500
 - 1s - loss: 0.6000 - accuracy: 0.7747 - val_loss: 0.6171 - val_accuracy: 0.7642
Epoch 19/7500
 - 1s - loss: 0.5926 - accuracy: 0.7758 - val_loss: 0.6122 - val_accuracy: 0.7650
Epoch 20/7500
 - 1s - loss: 0.5858 - accuracy: 0.7773 - val_loss: 0.6075 - val_accuracy: 0.7737
Epoch 21/7500
 - 1s - loss: 0.5789 - accuracy: 0.7802 - val_loss: 0.6024 - val_accuracy: 0.7745
Epoch 22/7500
 - 1s - loss: 0.5727 - accuracy: 0.7811 - val_loss: 0.5975 - val_accuracy: 0.7723
Epoch 23/7500
 - 1s - loss: 0.5665 - accuracy: 0.7833 - val_loss: 0.5932 - val_accuracy: 0.7752
Epoch 24/7500
 - 1s - loss: 0.5607 - accuracy: 0.7833 - val_loss: 0.5892 - val_accuracy: 0.7715
Epoch 25/7500
 - 1s - loss: 0.5554 - accuracy: 0.7862 - val_loss: 0.5854 - val_accuracy: 0.7730
Epoch 26/7500
 - 1s - loss: 0.5495 - accuracy: 0.7871 - val_loss: 0.5801 - val_accuracy: 0.7715
Epoch 27/7500
 - 1s - loss: 0.5444 - accuracy: 0.7890 - val_loss: 0.5775 - val_accuracy: 0.7723
Epoch 28/7500
 - 1s - loss: 0.5394 - accuracy: 0.7913 - val_loss: 0.5724 - val_accuracy: 0.7737
Epoch 29/7500
 - 1s - loss: 0.5354 - accuracy: 0.7935 - val_loss: 0.5682 - val_accuracy: 0.7759
Epoch 30/7500
 - 1s - loss: 0.5306 - accuracy: 0.7939 - val_loss: 0.5650 - val_accuracy: 0.7803
Epoch 31/7500
 - 1s - loss: 0.5260 - accuracy: 0.7976 - val_loss: 0.5618 - val_accuracy: 0.7796
Epoch 32/7500
 - 1s - loss: 0.5223 - accuracy: 0.7976 - val_loss: 0.5586 - val_accuracy: 0.7847
Epoch 33/7500
 - 1s - loss: 0.5177 - accuracy: 0.7992 - val_loss: 0.5556 - val_accuracy: 0.7869
Epoch 34/7500
 - 1s - loss: 0.5139 - accuracy: 0.8010 - val_loss: 0.5521 - val_accuracy: 0.7891
Epoch 35/7500
 - 1s - loss: 0.5096 - accuracy: 0.8025 - val_loss: 0.5491 - val_accuracy: 0.7876
Epoch 36/7500
 - 1s - loss: 0.5061 - accuracy: 0.8038 - val_loss: 0.5468 - val_accuracy: 0.7920
Epoch 37/7500
 - 1s - loss: 0.5020 - accuracy: 0.8070 - val_loss: 0.5439 - val_accuracy: 0.7934
Epoch 38/7500
 - 1s - loss: 0.4981 - accuracy: 0.8061 - val_loss: 0.5410 - val_accuracy: 0.7905
Epoch 39/7500
 - 1s - loss: 0.4948 - accuracy: 0.8092 - val_loss: 0.5405 - val_accuracy: 0.7912
Epoch 40/7500
 - 1s - loss: 0.4917 - accuracy: 0.8109 - val_loss: 0.5382 - val_accuracy: 0.7920
Epoch 41/7500
 - 1s - loss: 0.4888 - accuracy: 0.8118 - val_loss: 0.5379 - val_accuracy: 0.7905
Epoch 42/7500
 - 1s - loss: 0.4861 - accuracy: 0.8136 - val_loss: 0.5363 - val_accuracy: 0.7920
Epoch 43/7500
 - 1s - loss: 0.4829 - accuracy: 0.8147 - val_loss: 0.5337 - val_accuracy: 0.7942
Epoch 44/7500
 - 1s - loss: 0.4800 - accuracy: 0.8154 - val_loss: 0.5330 - val_accuracy: 0.7942
Epoch 45/7500
 - 1s - loss: 0.4774 - accuracy: 0.8160 - val_loss: 0.5295 - val_accuracy: 0.7978
Epoch 46/7500
 - 1s - loss: 0.4750 - accuracy: 0.8175 - val_loss: 0.5292 - val_accuracy: 0.7920
Epoch 47/7500
 - 1s - loss: 0.4723 - accuracy: 0.8182 - val_loss: 0.5283 - val_accuracy: 0.7927
Epoch 48/7500
 - 1s - loss: 0.4691 - accuracy: 0.8189 - val_loss: 0.5261 - val_accuracy: 0.7956
Epoch 49/7500
 - 1s - loss: 0.4674 - accuracy: 0.8206 - val_loss: 0.5266 - val_accuracy: 0.7956
Epoch 50/7500
 - 1s - loss: 0.4646 - accuracy: 0.8213 - val_loss: 0.5240 - val_accuracy: 0.7956
Epoch 51/7500
 - 1s - loss: 0.4624 - accuracy: 0.8238 - val_loss: 0.5233 - val_accuracy: 0.7942
Epoch 52/7500
 - 1s - loss: 0.4607 - accuracy: 0.8224 - val_loss: 0.5199 - val_accuracy: 0.7985
Epoch 53/7500
 - 1s - loss: 0.4586 - accuracy: 0.8237 - val_loss: 0.5198 - val_accuracy: 0.7993
Epoch 54/7500
 - 1s - loss: 0.4559 - accuracy: 0.8244 - val_loss: 0.5178 - val_accuracy: 0.8036
Epoch 55/7500
 - 1s - loss: 0.4547 - accuracy: 0.8255 - val_loss: 0.5174 - val_accuracy: 0.7971
Epoch 56/7500
 - 1s - loss: 0.4518 - accuracy: 0.8253 - val_loss: 0.5179 - val_accuracy: 0.8022
Epoch 57/7500
 - 1s - loss: 0.4495 - accuracy: 0.8264 - val_loss: 0.5145 - val_accuracy: 0.8036
Epoch 58/7500
 - 1s - loss: 0.4481 - accuracy: 0.8257 - val_loss: 0.5133 - val_accuracy: 0.8051
Epoch 59/7500
 - 1s - loss: 0.4454 - accuracy: 0.8264 - val_loss: 0.5101 - val_accuracy: 0.8036
Epoch 60/7500
 - 1s - loss: 0.4440 - accuracy: 0.8262 - val_loss: 0.5114 - val_accuracy: 0.8044
Epoch 61/7500
 - 1s - loss: 0.4423 - accuracy: 0.8271 - val_loss: 0.5102 - val_accuracy: 0.8066
Epoch 62/7500
 - 1s - loss: 0.4406 - accuracy: 0.8268 - val_loss: 0.5073 - val_accuracy: 0.8058
Epoch 63/7500
 - 1s - loss: 0.4385 - accuracy: 0.8275 - val_loss: 0.5072 - val_accuracy: 0.8073
Epoch 64/7500
 - 1s - loss: 0.4370 - accuracy: 0.8300 - val_loss: 0.5072 - val_accuracy: 0.8066
Epoch 65/7500
 - 1s - loss: 0.4349 - accuracy: 0.8291 - val_loss: 0.5065 - val_accuracy: 0.8073
Epoch 66/7500
 - 1s - loss: 0.4340 - accuracy: 0.8297 - val_loss: 0.5035 - val_accuracy: 0.8080
Epoch 67/7500
 - 1s - loss: 0.4321 - accuracy: 0.8297 - val_loss: 0.5008 - val_accuracy: 0.8080
Epoch 68/7500
 - 1s - loss: 0.4310 - accuracy: 0.8288 - val_loss: 0.5021 - val_accuracy: 0.8139
Epoch 69/7500
 - 1s - loss: 0.4287 - accuracy: 0.8308 - val_loss: 0.4977 - val_accuracy: 0.8117
Epoch 70/7500
 - 1s - loss: 0.4276 - accuracy: 0.8311 - val_loss: 0.4971 - val_accuracy: 0.8175
Epoch 71/7500
 - 1s - loss: 0.4263 - accuracy: 0.8328 - val_loss: 0.4989 - val_accuracy: 0.8197
Epoch 72/7500
 - 1s - loss: 0.4243 - accuracy: 0.8311 - val_loss: 0.4955 - val_accuracy: 0.8226
Epoch 73/7500
 - 1s - loss: 0.4232 - accuracy: 0.8339 - val_loss: 0.4943 - val_accuracy: 0.8204
Epoch 74/7500
 - 1s - loss: 0.4220 - accuracy: 0.8328 - val_loss: 0.4933 - val_accuracy: 0.8212
Epoch 75/7500
 - 1s - loss: 0.4189 - accuracy: 0.8335 - val_loss: 0.4915 - val_accuracy: 0.8255
Epoch 76/7500
 - 1s - loss: 0.4187 - accuracy: 0.8337 - val_loss: 0.4938 - val_accuracy: 0.8241
Epoch 77/7500
 - 1s - loss: 0.4169 - accuracy: 0.8348 - val_loss: 0.4941 - val_accuracy: 0.8234
Epoch 78/7500
 - 1s - loss: 0.4166 - accuracy: 0.8333 - val_loss: 0.4897 - val_accuracy: 0.8255
Epoch 79/7500
 - 1s - loss: 0.4141 - accuracy: 0.8342 - val_loss: 0.4890 - val_accuracy: 0.8263
Epoch 80/7500
 - 1s - loss: 0.4130 - accuracy: 0.8333 - val_loss: 0.4868 - val_accuracy: 0.8277
Epoch 81/7500
 - 1s - loss: 0.4116 - accuracy: 0.8339 - val_loss: 0.4883 - val_accuracy: 0.8248
Epoch 82/7500
 - 1s - loss: 0.4106 - accuracy: 0.8363 - val_loss: 0.4861 - val_accuracy: 0.8263
Epoch 83/7500
 - 1s - loss: 0.4097 - accuracy: 0.8357 - val_loss: 0.4869 - val_accuracy: 0.8277
Epoch 84/7500
 - 1s - loss: 0.4081 - accuracy: 0.8355 - val_loss: 0.4854 - val_accuracy: 0.8285
Epoch 85/7500
 - 1s - loss: 0.4073 - accuracy: 0.8370 - val_loss: 0.4845 - val_accuracy: 0.8292
Epoch 86/7500
 - 1s - loss: 0.4056 - accuracy: 0.8379 - val_loss: 0.4842 - val_accuracy: 0.8285
Epoch 87/7500
 - 1s - loss: 0.4045 - accuracy: 0.8386 - val_loss: 0.4806 - val_accuracy: 0.8248
Epoch 88/7500
 - 1s - loss: 0.4028 - accuracy: 0.8410 - val_loss: 0.4834 - val_accuracy: 0.8248
Epoch 89/7500
 - 1s - loss: 0.4009 - accuracy: 0.8405 - val_loss: 0.4805 - val_accuracy: 0.8285
Epoch 90/7500
 - 1s - loss: 0.3993 - accuracy: 0.8412 - val_loss: 0.4790 - val_accuracy: 0.8270
Epoch 91/7500
 - 1s - loss: 0.3979 - accuracy: 0.8408 - val_loss: 0.4809 - val_accuracy: 0.8270
Epoch 92/7500
 - 1s - loss: 0.3968 - accuracy: 0.8421 - val_loss: 0.4781 - val_accuracy: 0.8314
Epoch 93/7500
 - 1s - loss: 0.3952 - accuracy: 0.8414 - val_loss: 0.4769 - val_accuracy: 0.8263
Epoch 94/7500
 - 1s - loss: 0.3944 - accuracy: 0.8437 - val_loss: 0.4779 - val_accuracy: 0.8277
Epoch 95/7500
 - 1s - loss: 0.3928 - accuracy: 0.8437 - val_loss: 0.4766 - val_accuracy: 0.8292
Epoch 96/7500
 - 1s - loss: 0.3920 - accuracy: 0.8441 - val_loss: 0.4784 - val_accuracy: 0.8292
Epoch 97/7500
 - 1s - loss: 0.3917 - accuracy: 0.8443 - val_loss: 0.4754 - val_accuracy: 0.8307
Epoch 98/7500
 - 1s - loss: 0.3910 - accuracy: 0.8443 - val_loss: 0.4754 - val_accuracy: 0.8292
Epoch 99/7500
 - 1s - loss: 0.3901 - accuracy: 0.8437 - val_loss: 0.4751 - val_accuracy: 0.8307
Epoch 100/7500
 - 1s - loss: 0.3897 - accuracy: 0.8452 - val_loss: 0.4746 - val_accuracy: 0.8299
Epoch 101/7500
 - 1s - loss: 0.3892 - accuracy: 0.8441 - val_loss: 0.4777 - val_accuracy: 0.8248
Epoch 102/7500
 - 1s - loss: 0.3879 - accuracy: 0.8441 - val_loss: 0.4760 - val_accuracy: 0.8307
Epoch 103/7500
 - 1s - loss: 0.3877 - accuracy: 0.8437 - val_loss: 0.4775 - val_accuracy: 0.8285
Epoch 104/7500
 - 1s - loss: 0.3864 - accuracy: 0.8450 - val_loss: 0.4764 - val_accuracy: 0.8292
Epoch 105/7500
 - 1s - loss: 0.3857 - accuracy: 0.8445 - val_loss: 0.4760 - val_accuracy: 0.8299
Epoch 106/7500
 - 1s - loss: 0.3855 - accuracy: 0.8445 - val_loss: 0.4801 - val_accuracy: 0.8285
Epoch 107/7500
 - 1s - loss: 0.3845 - accuracy: 0.8445 - val_loss: 0.4773 - val_accuracy: 0.8277

Fit: epochs = 7500, batch_size = 80, verbose = 2, shuffle=False, validation_split = 0.20, callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)]

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 256)               274432    
_________________________________________________________________
dense_1 (Dense)              (None, 200)               51400     
_________________________________________________________________
dense_2 (Dense)              (None, 100)               20100     
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 404       
=================================================================
Total params: 346,336
Trainable params: 346,336
Non-trainable params: 0
_________________________________________________________________
None

Accuracy Train: 84.52%
Accuracy Test: 82.71%
Numero dati esaminati: 1712
True Positive 1416
False Positive 296
