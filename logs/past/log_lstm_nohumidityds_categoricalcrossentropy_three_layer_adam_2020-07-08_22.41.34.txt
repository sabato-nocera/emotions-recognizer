Dataset used: ../../datasets/full_dataset_without_humidity.csv 

   Temperature  Sound  Heartbeat   X1  ...    Y2     Z2  Classification  Feedback
0           32      1         60 -680  ... -7424 -15596             100     Happy
1           32      1         60 -780  ... -7408 -15628             100     Happy
2           -1      1         60   -1  ... -7276 -15612             100     Happy
3           -1     -1         60   -1  ...    -1     -1             100     Happy
4           32      1         60 -860  ... -7340 -15720             100     Happy

[5 rows x 11 columns]

Objservations: 8560
Reshaping:  ((6848, 10), (6848, 4), (1712, 10), (1712, 4))  -> ((6848, 1, 10), (6848, 4), (1712, 1, 10), (1712, 4))

Layers:

{'name': 'lstm_1', 'trainable': True, 'batch_input_shape': (None, 1, 10), 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'units': 500, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 2} 

{'name': 'lstm_2', 'trainable': True, 'batch_input_shape': (None, 1, 10), 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'units': 500, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 2} 

{'name': 'lstm_3', 'trainable': True, 'batch_input_shape': (None, 1, 10), 'dtype': 'float32', 'return_sequences': False, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'units': 500, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 2} 

{'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None} 

{'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 300, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 100, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 50, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 20, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

Compile: loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']

Start computation...

Train on 5478 samples, validate on 1370 samples
Epoch 1/128
 - 18s - loss: 1.0460 - accuracy: 0.5369 - val_loss: 0.8884 - val_accuracy: 0.6620
Epoch 2/128
 - 17s - loss: 0.8545 - accuracy: 0.6734 - val_loss: 0.8437 - val_accuracy: 0.6905
Epoch 3/128
 - 17s - loss: 0.8128 - accuracy: 0.6931 - val_loss: 0.8133 - val_accuracy: 0.7058
Epoch 4/128
 - 15s - loss: 0.7896 - accuracy: 0.7023 - val_loss: 0.7869 - val_accuracy: 0.7226
Epoch 5/128
 - 15s - loss: 0.7702 - accuracy: 0.7103 - val_loss: 0.7681 - val_accuracy: 0.7234
Epoch 6/128
 - 15s - loss: 0.7548 - accuracy: 0.7194 - val_loss: 0.7886 - val_accuracy: 0.7139
Epoch 7/128
 - 15s - loss: 0.7408 - accuracy: 0.7265 - val_loss: 0.7705 - val_accuracy: 0.7168
Epoch 8/128
 - 15s - loss: 0.7299 - accuracy: 0.7253 - val_loss: 0.7847 - val_accuracy: 0.7226
Epoch 9/128
 - 16s - loss: 0.7268 - accuracy: 0.7282 - val_loss: 0.7123 - val_accuracy: 0.7489
Epoch 10/128
 - 16s - loss: 0.7053 - accuracy: 0.7355 - val_loss: 0.7206 - val_accuracy: 0.7328
Epoch 11/128
 - 16s - loss: 0.6986 - accuracy: 0.7353 - val_loss: 0.7056 - val_accuracy: 0.7328
Epoch 12/128
 - 16s - loss: 0.6889 - accuracy: 0.7391 - val_loss: 0.7040 - val_accuracy: 0.7343
Epoch 13/128
 - 16s - loss: 0.6921 - accuracy: 0.7366 - val_loss: 0.6827 - val_accuracy: 0.7540
Epoch 14/128
 - 16s - loss: 0.6735 - accuracy: 0.7415 - val_loss: 0.6814 - val_accuracy: 0.7591
Epoch 15/128
 - 16s - loss: 0.6604 - accuracy: 0.7464 - val_loss: 0.6785 - val_accuracy: 0.7482
Epoch 16/128
 - 16s - loss: 0.6564 - accuracy: 0.7521 - val_loss: 0.6549 - val_accuracy: 0.7620
Epoch 17/128
 - 16s - loss: 0.6491 - accuracy: 0.7552 - val_loss: 0.6573 - val_accuracy: 0.7555
Epoch 18/128
 - 17s - loss: 0.6365 - accuracy: 0.7589 - val_loss: 0.6694 - val_accuracy: 0.7489
Epoch 19/128
 - 16s - loss: 0.6299 - accuracy: 0.7660 - val_loss: 0.6305 - val_accuracy: 0.7620
Epoch 20/128
 - 16s - loss: 0.6251 - accuracy: 0.7614 - val_loss: 0.6596 - val_accuracy: 0.7628
Epoch 21/128
 - 17s - loss: 0.6117 - accuracy: 0.7687 - val_loss: 0.6149 - val_accuracy: 0.7708
Epoch 22/128
 - 16s - loss: 0.6066 - accuracy: 0.7740 - val_loss: 0.6403 - val_accuracy: 0.7628
Epoch 23/128
 - 16s - loss: 0.6039 - accuracy: 0.7713 - val_loss: 0.6294 - val_accuracy: 0.7686
Epoch 24/128
 - 16s - loss: 0.5941 - accuracy: 0.7791 - val_loss: 0.6093 - val_accuracy: 0.7715
Epoch 25/128
 - 16s - loss: 0.5840 - accuracy: 0.7769 - val_loss: 0.5966 - val_accuracy: 0.7730
Epoch 26/128
 - 16s - loss: 0.5896 - accuracy: 0.7751 - val_loss: 0.6181 - val_accuracy: 0.7606
Epoch 27/128
 - 16s - loss: 0.5715 - accuracy: 0.7788 - val_loss: 0.6005 - val_accuracy: 0.7664
Epoch 28/128
 - 16s - loss: 0.5642 - accuracy: 0.7802 - val_loss: 0.6073 - val_accuracy: 0.7759
Epoch 29/128
 - 16s - loss: 0.5538 - accuracy: 0.7873 - val_loss: 0.5803 - val_accuracy: 0.7796
Epoch 30/128
 - 16s - loss: 0.5450 - accuracy: 0.7857 - val_loss: 0.5680 - val_accuracy: 0.7854
Epoch 31/128
 - 16s - loss: 0.5496 - accuracy: 0.7884 - val_loss: 0.5694 - val_accuracy: 0.7832
Epoch 32/128
 - 16s - loss: 0.5440 - accuracy: 0.7855 - val_loss: 0.5735 - val_accuracy: 0.7774
Epoch 33/128
 - 16s - loss: 0.5379 - accuracy: 0.7893 - val_loss: 0.5723 - val_accuracy: 0.7839
Epoch 34/128
 - 16s - loss: 0.5449 - accuracy: 0.7875 - val_loss: 0.5705 - val_accuracy: 0.7883
Epoch 35/128
 - 16s - loss: 0.5301 - accuracy: 0.7934 - val_loss: 0.5706 - val_accuracy: 0.7861
Epoch 36/128
 - 16s - loss: 0.5347 - accuracy: 0.7871 - val_loss: 0.5565 - val_accuracy: 0.7723
Epoch 37/128
 - 16s - loss: 0.5156 - accuracy: 0.7979 - val_loss: 0.5431 - val_accuracy: 0.7854
Epoch 38/128
 - 17s - loss: 0.5049 - accuracy: 0.7965 - val_loss: 0.5332 - val_accuracy: 0.7949
Epoch 39/128
 - 17s - loss: 0.5125 - accuracy: 0.7939 - val_loss: 0.5356 - val_accuracy: 0.7927
Epoch 40/128
 - 16s - loss: 0.4869 - accuracy: 0.8058 - val_loss: 0.5501 - val_accuracy: 0.7766
Epoch 41/128
 - 17s - loss: 0.5045 - accuracy: 0.8014 - val_loss: 0.5571 - val_accuracy: 0.7810
Epoch 42/128
 - 16s - loss: 0.4844 - accuracy: 0.8120 - val_loss: 0.5399 - val_accuracy: 0.8080
Epoch 43/128
 - 16s - loss: 0.4808 - accuracy: 0.8145 - val_loss: 0.5501 - val_accuracy: 0.7934
Epoch 44/128
 - 16s - loss: 0.4719 - accuracy: 0.8138 - val_loss: 0.5438 - val_accuracy: 0.7993
Epoch 45/128
 - 16s - loss: 0.4868 - accuracy: 0.8109 - val_loss: 0.5355 - val_accuracy: 0.7927
Epoch 46/128
 - 16s - loss: 0.4818 - accuracy: 0.8129 - val_loss: 0.5385 - val_accuracy: 0.8066
Epoch 47/128
 - 16s - loss: 0.4570 - accuracy: 0.8213 - val_loss: 0.5545 - val_accuracy: 0.8029
Epoch 48/128
 - 16s - loss: 0.4646 - accuracy: 0.8187 - val_loss: 0.5001 - val_accuracy: 0.8139
Epoch 49/128
 - 16s - loss: 0.4628 - accuracy: 0.8184 - val_loss: 0.5012 - val_accuracy: 0.8124
Epoch 50/128
 - 16s - loss: 0.4532 - accuracy: 0.8227 - val_loss: 0.5295 - val_accuracy: 0.8088
Epoch 51/128
 - 16s - loss: 0.4552 - accuracy: 0.8189 - val_loss: 0.5005 - val_accuracy: 0.8095
Epoch 52/128
 - 16s - loss: 0.4434 - accuracy: 0.8260 - val_loss: 0.5216 - val_accuracy: 0.8029
Epoch 53/128
 - 16s - loss: 0.4383 - accuracy: 0.8240 - val_loss: 0.4888 - val_accuracy: 0.8161
Epoch 54/128
 - 16s - loss: 0.4388 - accuracy: 0.8279 - val_loss: 0.5083 - val_accuracy: 0.8088
Epoch 55/128
 - 16s - loss: 0.4587 - accuracy: 0.8206 - val_loss: 0.4844 - val_accuracy: 0.8190
Epoch 56/128
 - 16s - loss: 0.4539 - accuracy: 0.8224 - val_loss: 0.5489 - val_accuracy: 0.8000
Epoch 57/128
 - 16s - loss: 0.4455 - accuracy: 0.8273 - val_loss: 0.4710 - val_accuracy: 0.8204
Epoch 58/128
 - 16s - loss: 0.4334 - accuracy: 0.8271 - val_loss: 0.4934 - val_accuracy: 0.8109
Epoch 59/128
 - 16s - loss: 0.4346 - accuracy: 0.8251 - val_loss: 0.5021 - val_accuracy: 0.8146
Epoch 60/128
 - 16s - loss: 0.4196 - accuracy: 0.8288 - val_loss: 0.4846 - val_accuracy: 0.8270
Epoch 61/128
 - 16s - loss: 0.4330 - accuracy: 0.8253 - val_loss: 0.4954 - val_accuracy: 0.8146
Epoch 62/128
 - 16s - loss: 0.4322 - accuracy: 0.8275 - val_loss: 0.4963 - val_accuracy: 0.8124
Epoch 63/128
 - 16s - loss: 0.4219 - accuracy: 0.8324 - val_loss: 0.5287 - val_accuracy: 0.8058
Epoch 64/128
 - 16s - loss: 0.4239 - accuracy: 0.8308 - val_loss: 0.5168 - val_accuracy: 0.8270
Epoch 65/128
 - 16s - loss: 0.4197 - accuracy: 0.8317 - val_loss: 0.4912 - val_accuracy: 0.8204
Epoch 66/128
 - 16s - loss: 0.4169 - accuracy: 0.8306 - val_loss: 0.5336 - val_accuracy: 0.8212
Epoch 67/128
 - 16s - loss: 0.4212 - accuracy: 0.8308 - val_loss: 0.5173 - val_accuracy: 0.8139
Epoch 68/128
 - 16s - loss: 0.4084 - accuracy: 0.8344 - val_loss: 0.4955 - val_accuracy: 0.8285
Epoch 69/128
 - 16s - loss: 0.4147 - accuracy: 0.8361 - val_loss: 0.5077 - val_accuracy: 0.8073
Epoch 70/128
 - 16s - loss: 0.4104 - accuracy: 0.8335 - val_loss: 0.5013 - val_accuracy: 0.8226
Epoch 71/128
 - 16s - loss: 0.4072 - accuracy: 0.8337 - val_loss: 0.5238 - val_accuracy: 0.8058
Epoch 72/128
 - 16s - loss: 0.4140 - accuracy: 0.8324 - val_loss: 0.4883 - val_accuracy: 0.8248
Epoch 73/128
 - 16s - loss: 0.4070 - accuracy: 0.8386 - val_loss: 0.5179 - val_accuracy: 0.8131
Epoch 74/128
 - 16s - loss: 0.4350 - accuracy: 0.8248 - val_loss: 0.5251 - val_accuracy: 0.8117
Epoch 75/128
 - 16s - loss: 0.4046 - accuracy: 0.8363 - val_loss: 0.4887 - val_accuracy: 0.8299
Epoch 76/128
 - 16s - loss: 0.4167 - accuracy: 0.8342 - val_loss: 0.4634 - val_accuracy: 0.8336
Epoch 77/128
 - 16s - loss: 0.4053 - accuracy: 0.8353 - val_loss: 0.4949 - val_accuracy: 0.8234
Epoch 78/128
 - 16s - loss: 0.3973 - accuracy: 0.8366 - val_loss: 0.4967 - val_accuracy: 0.8270
Epoch 79/128
 - 16s - loss: 0.3893 - accuracy: 0.8386 - val_loss: 0.5063 - val_accuracy: 0.8219
Epoch 80/128
 - 16s - loss: 0.3865 - accuracy: 0.8415 - val_loss: 0.4752 - val_accuracy: 0.8255
Epoch 81/128
 - 16s - loss: 0.3885 - accuracy: 0.8383 - val_loss: 0.5127 - val_accuracy: 0.8248
Epoch 82/128
 - 16s - loss: 0.3935 - accuracy: 0.8412 - val_loss: 0.5042 - val_accuracy: 0.8219
Epoch 83/128
 - 16s - loss: 0.4115 - accuracy: 0.8342 - val_loss: 0.4750 - val_accuracy: 0.8321
Epoch 84/128
 - 16s - loss: 0.3920 - accuracy: 0.8403 - val_loss: 0.5278 - val_accuracy: 0.8292
Epoch 85/128
 - 16s - loss: 0.3846 - accuracy: 0.8412 - val_loss: 0.5082 - val_accuracy: 0.8204
Epoch 86/128
 - 16s - loss: 0.3917 - accuracy: 0.8406 - val_loss: 0.4670 - val_accuracy: 0.8372
Epoch 87/128
 - 16s - loss: 0.3875 - accuracy: 0.8394 - val_loss: 0.5335 - val_accuracy: 0.8139
Epoch 88/128
 - 16s - loss: 0.3841 - accuracy: 0.8430 - val_loss: 0.4776 - val_accuracy: 0.8416
Epoch 89/128
 - 17s - loss: 0.3752 - accuracy: 0.8423 - val_loss: 0.4714 - val_accuracy: 0.8307
Epoch 90/128
 - 17s - loss: 0.3793 - accuracy: 0.8434 - val_loss: 0.4808 - val_accuracy: 0.8328
Epoch 91/128
 - 17s - loss: 0.3755 - accuracy: 0.8448 - val_loss: 0.4809 - val_accuracy: 0.8394
Epoch 92/128
 - 16s - loss: 0.3713 - accuracy: 0.8507 - val_loss: 0.4692 - val_accuracy: 0.8299
Epoch 93/128
 - 19s - loss: 0.3771 - accuracy: 0.8426 - val_loss: 0.5005 - val_accuracy: 0.8350
Epoch 94/128
 - 17s - loss: 0.3759 - accuracy: 0.8428 - val_loss: 0.4806 - val_accuracy: 0.8321
Epoch 95/128
 - 17s - loss: 0.3697 - accuracy: 0.8439 - val_loss: 0.4695 - val_accuracy: 0.8394
Epoch 96/128
 - 17s - loss: 0.3747 - accuracy: 0.8481 - val_loss: 0.4698 - val_accuracy: 0.8431
Epoch 97/128
 - 17s - loss: 0.3802 - accuracy: 0.8447 - val_loss: 0.4587 - val_accuracy: 0.8350
Epoch 98/128
 - 18s - loss: 0.3744 - accuracy: 0.8459 - val_loss: 0.4782 - val_accuracy: 0.8453
Epoch 99/128
 - 17s - loss: 0.3601 - accuracy: 0.8499 - val_loss: 0.4886 - val_accuracy: 0.8365
Epoch 100/128
 - 17s - loss: 0.3573 - accuracy: 0.8543 - val_loss: 0.4707 - val_accuracy: 0.8336
Epoch 101/128
 - 17s - loss: 0.3876 - accuracy: 0.8448 - val_loss: 0.4874 - val_accuracy: 0.8328
Epoch 102/128
 - 16s - loss: 0.3606 - accuracy: 0.8518 - val_loss: 0.4921 - val_accuracy: 0.8102
Epoch 103/128
 - 17s - loss: 0.3825 - accuracy: 0.8439 - val_loss: 0.4634 - val_accuracy: 0.8314
Epoch 104/128
 - 16s - loss: 0.3697 - accuracy: 0.8481 - val_loss: 0.4909 - val_accuracy: 0.8365
Epoch 105/128
 - 17s - loss: 0.3655 - accuracy: 0.8472 - val_loss: 0.4675 - val_accuracy: 0.8321
Epoch 106/128
 - 17s - loss: 0.3655 - accuracy: 0.8452 - val_loss: 0.4792 - val_accuracy: 0.8387
Epoch 107/128
 - 17s - loss: 0.3514 - accuracy: 0.8510 - val_loss: 0.4775 - val_accuracy: 0.8482
Epoch 108/128
 - 17s - loss: 0.3488 - accuracy: 0.8534 - val_loss: 0.4661 - val_accuracy: 0.8467
Epoch 109/128
 - 20s - loss: 0.3496 - accuracy: 0.8556 - val_loss: 0.4782 - val_accuracy: 0.8328
Epoch 110/128
 - 18s - loss: 0.3563 - accuracy: 0.8505 - val_loss: 0.4800 - val_accuracy: 0.8423
Epoch 111/128
 - 19s - loss: 0.3596 - accuracy: 0.8483 - val_loss: 0.5041 - val_accuracy: 0.8182
Epoch 112/128
 - 17s - loss: 0.3625 - accuracy: 0.8479 - val_loss: 0.5216 - val_accuracy: 0.8204
Epoch 113/128
 - 17s - loss: 0.3512 - accuracy: 0.8545 - val_loss: 0.4705 - val_accuracy: 0.8431
Epoch 114/128
 - 17s - loss: 0.3549 - accuracy: 0.8530 - val_loss: 0.5117 - val_accuracy: 0.8292
Epoch 115/128
 - 17s - loss: 0.3508 - accuracy: 0.8565 - val_loss: 0.4865 - val_accuracy: 0.8453
Epoch 116/128
 - 17s - loss: 0.3505 - accuracy: 0.8512 - val_loss: 0.4764 - val_accuracy: 0.8401
Epoch 117/128
 - 17s - loss: 0.3400 - accuracy: 0.8551 - val_loss: 0.5221 - val_accuracy: 0.8212
Epoch 118/128
 - 17s - loss: 0.3567 - accuracy: 0.8530 - val_loss: 0.4843 - val_accuracy: 0.8431
Epoch 119/128
 - 16s - loss: 0.3672 - accuracy: 0.8472 - val_loss: 0.4955 - val_accuracy: 0.8365
Epoch 120/128
 - 16s - loss: 0.3463 - accuracy: 0.8532 - val_loss: 0.4801 - val_accuracy: 0.8409
Epoch 121/128
 - 17s - loss: 0.3568 - accuracy: 0.8476 - val_loss: 0.5102 - val_accuracy: 0.8299
Epoch 122/128
 - 17s - loss: 0.3407 - accuracy: 0.8558 - val_loss: 0.5256 - val_accuracy: 0.8358
Epoch 123/128
 - 17s - loss: 0.3590 - accuracy: 0.8461 - val_loss: 0.5121 - val_accuracy: 0.8460
Epoch 124/128
 - 17s - loss: 0.3454 - accuracy: 0.8569 - val_loss: 0.4969 - val_accuracy: 0.8489
Epoch 125/128
 - 17s - loss: 0.3401 - accuracy: 0.8631 - val_loss: 0.5193 - val_accuracy: 0.8350
Epoch 126/128
 - 17s - loss: 0.3416 - accuracy: 0.8552 - val_loss: 0.5037 - val_accuracy: 0.8445
Epoch 127/128
 - 17s - loss: 0.3415 - accuracy: 0.8551 - val_loss: 0.4990 - val_accuracy: 0.8511
Epoch 128/128
 - 17s - loss: 0.3440 - accuracy: 0.8571 - val_loss: 0.4750 - val_accuracy: 0.8453

Fit: epochs= 128 , batch_size= 32 , verbose= 2 , shuffle= False , validation_split= 0.2 

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 1, 500)            1022000   
_________________________________________________________________
lstm_2 (LSTM)                (None, 1, 500)            2002000   
_________________________________________________________________
lstm_3 (LSTM)                (None, 500)               2002000   
_________________________________________________________________
dropout_1 (Dropout)          (None, 500)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 300)               150300    
_________________________________________________________________
dense_2 (Dense)              (None, 200)               60200     
_________________________________________________________________
dense_3 (Dense)              (None, 100)               20100     
_________________________________________________________________
dense_4 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_5 (Dense)              (None, 20)                1020      
_________________________________________________________________
dense_6 (Dense)              (None, 4)                 84        
=================================================================
Total params: 5,262,754
Trainable params: 5,262,754
Non-trainable params: 0
_________________________________________________________________
None

Accuracy Train: 85.50%
Accuracy Test: 83.18%
Loss Train: 0.36
Loss Test: 0.47
Numero dati esaminati: 1712
True Positive 1424
False Positive 288
