Dataset used: ../../datasets/other/train_dataset_for_augmentation.csv 

   Unnamed: 0  Temperature  Sound  ...     Z2  Classification  Feedback
0           0         32.0      1  ... -15596             100     Happy
1           1         32.0      1  ... -15628             100     Happy
2           2         -1.0      1  ... -15612             100     Happy
3           3         -1.0     -1  ...     -1             100     Happy
4           4         32.0      1  ... -15720             100     Happy

[5 rows x 12 columns]

Objservations: 12840
Dataset used: ../../datasets/other/test_dataset_for_augmentation.csv 

   Temperature  Sound  Heartbeat   X1  ...    Y2     Z2  Classification  Feedback
0           35      1         64  844  ... -7000 -15764             250       Sad
1           35     -1         64  832  ...    -1     -1             250       Sad
2           35      1         64  768  ... -7000 -15800             250       Sad
3           -1      1         64   -1  ... -7168 -15892             250       Sad
4           35     -1         64  692  ...    -1     -1             250       Sad

[5 rows x 11 columns]

Objservations: 4280

Layers:

{'name': 'dense_1', 'trainable': True, 'batch_input_shape': (None, 10), 'dtype': 'float32', 'units': 10, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 500, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 300, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 100, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 50, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 20, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

Compile: loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']

Start computation...

Train on 10272 samples, validate on 2568 samples
Epoch 1/128
 - 2s - loss: 1.3849 - accuracy: 0.2784 - val_loss: 1.3850 - val_accuracy: 0.2699
Epoch 2/128
 - 2s - loss: 1.3809 - accuracy: 0.2941 - val_loss: 1.3858 - val_accuracy: 0.2753
Epoch 3/128
 - 2s - loss: 1.3789 - accuracy: 0.2969 - val_loss: 1.3866 - val_accuracy: 0.2730
Epoch 4/128
 - 2s - loss: 1.3780 - accuracy: 0.2968 - val_loss: 1.3871 - val_accuracy: 0.2726
Epoch 5/128
 - 2s - loss: 1.3775 - accuracy: 0.2985 - val_loss: 1.3878 - val_accuracy: 0.2722
Epoch 6/128
 - 2s - loss: 1.3766 - accuracy: 0.3020 - val_loss: 1.3928 - val_accuracy: 0.2465
Epoch 7/128
 - 1s - loss: 1.3759 - accuracy: 0.3052 - val_loss: 1.3952 - val_accuracy: 0.2465
Epoch 8/128
 - 2s - loss: 1.3772 - accuracy: 0.3000 - val_loss: 1.3902 - val_accuracy: 0.2691
Epoch 9/128
 - 2s - loss: 1.3751 - accuracy: 0.3055 - val_loss: 1.3941 - val_accuracy: 0.2488
Epoch 10/128
 - 2s - loss: 1.3751 - accuracy: 0.2984 - val_loss: 1.3879 - val_accuracy: 0.2738
Epoch 11/128
 - 2s - loss: 1.3735 - accuracy: 0.3065 - val_loss: 1.3981 - val_accuracy: 0.2500
Epoch 12/128
 - 1s - loss: 1.3729 - accuracy: 0.3059 - val_loss: 1.3989 - val_accuracy: 0.2516
Epoch 13/128
 - 2s - loss: 1.3717 - accuracy: 0.3072 - val_loss: 1.3924 - val_accuracy: 0.2667
Epoch 14/128
 - 2s - loss: 1.3705 - accuracy: 0.3112 - val_loss: 1.3913 - val_accuracy: 0.2691
Epoch 15/128
 - 2s - loss: 1.3681 - accuracy: 0.3137 - val_loss: 1.3919 - val_accuracy: 0.2656
Epoch 16/128
 - 1s - loss: 1.3677 - accuracy: 0.3153 - val_loss: 1.3904 - val_accuracy: 0.2691
Epoch 17/128
 - 2s - loss: 1.3651 - accuracy: 0.3139 - val_loss: 1.3916 - val_accuracy: 0.2675
Epoch 18/128
 - 2s - loss: 1.3626 - accuracy: 0.3255 - val_loss: 1.4046 - val_accuracy: 0.2469
Epoch 19/128
 - 2s - loss: 1.3648 - accuracy: 0.3190 - val_loss: 1.3923 - val_accuracy: 0.2687
Epoch 20/128
 - 2s - loss: 1.3625 - accuracy: 0.3246 - val_loss: 1.3943 - val_accuracy: 0.2671
Epoch 21/128
 - 2s - loss: 1.3567 - accuracy: 0.3266 - val_loss: 1.3945 - val_accuracy: 0.2652
Epoch 22/128
 - 2s - loss: 1.3543 - accuracy: 0.3295 - val_loss: 1.3965 - val_accuracy: 0.2679
Epoch 23/128
 - 2s - loss: 1.3516 - accuracy: 0.3286 - val_loss: 1.4082 - val_accuracy: 0.2652
Epoch 24/128
 - 2s - loss: 1.3508 - accuracy: 0.3286 - val_loss: 1.3981 - val_accuracy: 0.2597
Epoch 25/128
 - 2s - loss: 1.3467 - accuracy: 0.3369 - val_loss: 1.3977 - val_accuracy: 0.2566
Epoch 26/128
 - 2s - loss: 1.3420 - accuracy: 0.3437 - val_loss: 1.3930 - val_accuracy: 0.2718
Epoch 27/128
 - 2s - loss: 1.3389 - accuracy: 0.3378 - val_loss: 1.3964 - val_accuracy: 0.2687
Epoch 28/128
 - 2s - loss: 1.3347 - accuracy: 0.3525 - val_loss: 1.3949 - val_accuracy: 0.2695
Epoch 29/128
 - 2s - loss: 1.3283 - accuracy: 0.3558 - val_loss: 1.4017 - val_accuracy: 0.2640
Epoch 30/128
 - 2s - loss: 1.3304 - accuracy: 0.3502 - val_loss: 1.3982 - val_accuracy: 0.2699
Epoch 31/128
 - 2s - loss: 1.3237 - accuracy: 0.3491 - val_loss: 1.4039 - val_accuracy: 0.2632
Epoch 32/128
 - 2s - loss: 1.3186 - accuracy: 0.3507 - val_loss: 1.4037 - val_accuracy: 0.2675
Epoch 33/128
 - 2s - loss: 1.3158 - accuracy: 0.3578 - val_loss: 1.4070 - val_accuracy: 0.2699
Epoch 34/128
 - 1s - loss: 1.3140 - accuracy: 0.3596 - val_loss: 1.4096 - val_accuracy: 0.2636
Epoch 35/128
 - 1s - loss: 1.3059 - accuracy: 0.3642 - val_loss: 1.4161 - val_accuracy: 0.2593
Epoch 36/128
 - 1s - loss: 1.3077 - accuracy: 0.3617 - val_loss: 1.4210 - val_accuracy: 0.2636
Epoch 37/128
 - 2s - loss: 1.2987 - accuracy: 0.3714 - val_loss: 1.4210 - val_accuracy: 0.2593
Epoch 38/128
 - 2s - loss: 1.2955 - accuracy: 0.3755 - val_loss: 1.4209 - val_accuracy: 0.2621
Epoch 39/128
 - 2s - loss: 1.2967 - accuracy: 0.3675 - val_loss: 1.4197 - val_accuracy: 0.2601
Epoch 40/128
 - 2s - loss: 1.2849 - accuracy: 0.3817 - val_loss: 1.4293 - val_accuracy: 0.2613
Epoch 41/128
 - 2s - loss: 1.2822 - accuracy: 0.3817 - val_loss: 1.4215 - val_accuracy: 0.2617
Epoch 42/128
 - 1s - loss: 1.2864 - accuracy: 0.3780 - val_loss: 1.4273 - val_accuracy: 0.2617
Epoch 43/128
 - 2s - loss: 1.2735 - accuracy: 0.3915 - val_loss: 1.4448 - val_accuracy: 0.2597
Epoch 44/128
 - 2s - loss: 1.2682 - accuracy: 0.3911 - val_loss: 1.4488 - val_accuracy: 0.2593
Epoch 45/128
 - 2s - loss: 1.2678 - accuracy: 0.3972 - val_loss: 1.4768 - val_accuracy: 0.2609
Epoch 46/128
 - 2s - loss: 1.2693 - accuracy: 0.3959 - val_loss: 1.4716 - val_accuracy: 0.2570
Epoch 47/128
 - 2s - loss: 1.2750 - accuracy: 0.3857 - val_loss: 1.5545 - val_accuracy: 0.2644
Epoch 48/128
 - 1s - loss: 1.2703 - accuracy: 0.3975 - val_loss: 1.5002 - val_accuracy: 0.2667
Epoch 49/128
 - 2s - loss: 1.2601 - accuracy: 0.3965 - val_loss: 1.5670 - val_accuracy: 0.2652
Epoch 50/128
 - 1s - loss: 1.2595 - accuracy: 0.4005 - val_loss: 1.5889 - val_accuracy: 0.2617
Epoch 51/128
 - 2s - loss: 1.2572 - accuracy: 0.3994 - val_loss: 1.5940 - val_accuracy: 0.2609
Epoch 52/128
 - 3s - loss: 1.2503 - accuracy: 0.4001 - val_loss: 1.6513 - val_accuracy: 0.2675
Epoch 53/128
 - 2s - loss: 1.2528 - accuracy: 0.3978 - val_loss: 1.6098 - val_accuracy: 0.2566
Epoch 54/128
 - 2s - loss: 1.2546 - accuracy: 0.4021 - val_loss: 1.6615 - val_accuracy: 0.2601
Epoch 55/128
 - 2s - loss: 1.2557 - accuracy: 0.3914 - val_loss: 1.6983 - val_accuracy: 0.2625
Epoch 56/128
 - 1s - loss: 1.2369 - accuracy: 0.4117 - val_loss: 1.7641 - val_accuracy: 0.2621
Epoch 57/128
 - 2s - loss: 1.2656 - accuracy: 0.3904 - val_loss: 1.7696 - val_accuracy: 0.2621
Epoch 58/128
 - 2s - loss: 1.2385 - accuracy: 0.4114 - val_loss: 1.7578 - val_accuracy: 0.2648
Epoch 59/128
 - 2s - loss: 1.2383 - accuracy: 0.4088 - val_loss: 1.7308 - val_accuracy: 0.2617
Epoch 60/128
 - 2s - loss: 1.2183 - accuracy: 0.4231 - val_loss: 1.8466 - val_accuracy: 0.2613
Epoch 61/128
 - 2s - loss: 1.2164 - accuracy: 0.4238 - val_loss: 1.8027 - val_accuracy: 0.2605
Epoch 62/128
 - 2s - loss: 1.2130 - accuracy: 0.4284 - val_loss: 1.9439 - val_accuracy: 0.2648
Epoch 63/128
 - 2s - loss: 1.2304 - accuracy: 0.4090 - val_loss: 1.7698 - val_accuracy: 0.2609
Epoch 64/128
 - 2s - loss: 1.2138 - accuracy: 0.4251 - val_loss: 2.0811 - val_accuracy: 0.2566
Epoch 65/128
 - 2s - loss: 1.2097 - accuracy: 0.4233 - val_loss: 2.1673 - val_accuracy: 0.2617
Epoch 66/128
 - 2s - loss: 1.2103 - accuracy: 0.4236 - val_loss: 2.0564 - val_accuracy: 0.2617
Epoch 67/128
 - 2s - loss: 1.1988 - accuracy: 0.4353 - val_loss: 2.2240 - val_accuracy: 0.2687
Epoch 68/128
 - 2s - loss: 1.2126 - accuracy: 0.4303 - val_loss: 2.0146 - val_accuracy: 0.2590
Epoch 69/128
 - 2s - loss: 1.2045 - accuracy: 0.4269 - val_loss: 1.9522 - val_accuracy: 0.2648
Epoch 70/128
 - 2s - loss: 1.1990 - accuracy: 0.4334 - val_loss: 1.9484 - val_accuracy: 0.2613
Epoch 71/128
 - 2s - loss: 1.1892 - accuracy: 0.4388 - val_loss: 2.3417 - val_accuracy: 0.2582
Epoch 72/128
 - 1s - loss: 1.1847 - accuracy: 0.4399 - val_loss: 2.5452 - val_accuracy: 0.2590
Epoch 73/128
 - 2s - loss: 1.1883 - accuracy: 0.4395 - val_loss: 2.5468 - val_accuracy: 0.2617
Epoch 74/128
 - 1s - loss: 1.1929 - accuracy: 0.4405 - val_loss: 2.4964 - val_accuracy: 0.2590
Epoch 75/128
 - 2s - loss: 1.1843 - accuracy: 0.4404 - val_loss: 2.3638 - val_accuracy: 0.2609
Epoch 76/128
 - 2s - loss: 1.1891 - accuracy: 0.4426 - val_loss: 2.3797 - val_accuracy: 0.2640
Epoch 77/128
 - 2s - loss: 1.2035 - accuracy: 0.4313 - val_loss: 2.4213 - val_accuracy: 0.2625
Epoch 78/128
 - 2s - loss: 1.1841 - accuracy: 0.4350 - val_loss: 2.6838 - val_accuracy: 0.2597
Epoch 79/128
 - 2s - loss: 1.1573 - accuracy: 0.4600 - val_loss: 2.9155 - val_accuracy: 0.2586
Epoch 80/128
 - 2s - loss: 1.1731 - accuracy: 0.4495 - val_loss: 2.9641 - val_accuracy: 0.2644
Epoch 81/128
 - 2s - loss: 1.1626 - accuracy: 0.4484 - val_loss: 2.9438 - val_accuracy: 0.2648
Epoch 82/128
 - 1s - loss: 1.1544 - accuracy: 0.4626 - val_loss: 2.9980 - val_accuracy: 0.2714
Epoch 83/128
 - 1s - loss: 1.1523 - accuracy: 0.4577 - val_loss: 2.6747 - val_accuracy: 0.2656
Epoch 84/128
 - 1s - loss: 1.1682 - accuracy: 0.4494 - val_loss: 2.6747 - val_accuracy: 0.2667
Epoch 85/128
 - 2s - loss: 1.1950 - accuracy: 0.4393 - val_loss: 2.4610 - val_accuracy: 0.2582
Epoch 86/128
 - 1s - loss: 1.1571 - accuracy: 0.4534 - val_loss: 2.1115 - val_accuracy: 0.2558
Epoch 87/128
 - 1s - loss: 1.1546 - accuracy: 0.4568 - val_loss: 1.8693 - val_accuracy: 0.2769
Epoch 88/128
 - 2s - loss: 1.1632 - accuracy: 0.4492 - val_loss: 2.0882 - val_accuracy: 0.2726
Epoch 89/128
 - 2s - loss: 1.1447 - accuracy: 0.4622 - val_loss: 2.4519 - val_accuracy: 0.2702
Epoch 90/128
 - 2s - loss: 1.1304 - accuracy: 0.4648 - val_loss: 2.4064 - val_accuracy: 0.2695
Epoch 91/128
 - 1s - loss: 1.1270 - accuracy: 0.4721 - val_loss: 3.1506 - val_accuracy: 0.2621
Epoch 92/128
 - 1s - loss: 1.1257 - accuracy: 0.4731 - val_loss: 3.0914 - val_accuracy: 0.2597
Epoch 93/128
 - 1s - loss: 1.1256 - accuracy: 0.4719 - val_loss: 2.3028 - val_accuracy: 0.2734
Epoch 94/128
 - 1s - loss: 1.1301 - accuracy: 0.4744 - val_loss: 2.1558 - val_accuracy: 0.2636
Epoch 95/128
 - 1s - loss: 1.1370 - accuracy: 0.4625 - val_loss: 2.4310 - val_accuracy: 0.2593
Epoch 96/128
 - 2s - loss: 1.1348 - accuracy: 0.4719 - val_loss: 2.2437 - val_accuracy: 0.2632
Epoch 97/128
 - 2s - loss: 1.1157 - accuracy: 0.4764 - val_loss: 2.7606 - val_accuracy: 0.2660
Epoch 98/128
 - 2s - loss: 1.0885 - accuracy: 0.4909 - val_loss: 2.8581 - val_accuracy: 0.2574
Epoch 99/128
 - 2s - loss: 1.0972 - accuracy: 0.4811 - val_loss: 2.6646 - val_accuracy: 0.2531
Epoch 100/128
 - 1s - loss: 1.0931 - accuracy: 0.4883 - val_loss: 2.8637 - val_accuracy: 0.2617
Epoch 101/128
 - 2s - loss: 1.0874 - accuracy: 0.4879 - val_loss: 2.4246 - val_accuracy: 0.2582
Epoch 102/128
 - 2s - loss: 1.0913 - accuracy: 0.4889 - val_loss: 2.9083 - val_accuracy: 0.2551
Epoch 103/128
 - 2s - loss: 1.1164 - accuracy: 0.4792 - val_loss: 2.6680 - val_accuracy: 0.2562
Epoch 104/128
 - 1s - loss: 1.1055 - accuracy: 0.4867 - val_loss: 2.5453 - val_accuracy: 0.2543
Epoch 105/128
 - 2s - loss: 1.0946 - accuracy: 0.4894 - val_loss: 2.7472 - val_accuracy: 0.2640
Epoch 106/128
 - 2s - loss: 1.0750 - accuracy: 0.4975 - val_loss: 2.6147 - val_accuracy: 0.2593
Epoch 107/128
 - 2s - loss: 1.0815 - accuracy: 0.4982 - val_loss: 2.5001 - val_accuracy: 0.2523
Epoch 108/128
 - 2s - loss: 1.0710 - accuracy: 0.5000 - val_loss: 3.0353 - val_accuracy: 0.2516
Epoch 109/128
 - 2s - loss: 1.0811 - accuracy: 0.4979 - val_loss: 2.6375 - val_accuracy: 0.2504
Epoch 110/128
 - 2s - loss: 1.1067 - accuracy: 0.4819 - val_loss: 2.7461 - val_accuracy: 0.2597
Epoch 111/128
 - 2s - loss: 1.0877 - accuracy: 0.4985 - val_loss: 2.5832 - val_accuracy: 0.2593
Epoch 112/128
 - 2s - loss: 1.0614 - accuracy: 0.5149 - val_loss: 2.7054 - val_accuracy: 0.2504
Epoch 113/128
 - 1s - loss: 1.0548 - accuracy: 0.5156 - val_loss: 2.8257 - val_accuracy: 0.2578
Epoch 114/128
 - 2s - loss: 1.0692 - accuracy: 0.5110 - val_loss: 2.8635 - val_accuracy: 0.2469
Epoch 115/128
 - 2s - loss: 1.0499 - accuracy: 0.5092 - val_loss: 2.4848 - val_accuracy: 0.2488
Epoch 116/128
 - 1s - loss: 1.0685 - accuracy: 0.5012 - val_loss: 2.5141 - val_accuracy: 0.2543
Epoch 117/128
 - 1s - loss: 1.0717 - accuracy: 0.5114 - val_loss: 3.2546 - val_accuracy: 0.2527
Epoch 118/128
 - 2s - loss: 1.0548 - accuracy: 0.5138 - val_loss: 2.5497 - val_accuracy: 0.2539
Epoch 119/128
 - 2s - loss: 1.0377 - accuracy: 0.5204 - val_loss: 2.6610 - val_accuracy: 0.2597
Epoch 120/128
 - 2s - loss: 1.0331 - accuracy: 0.5228 - val_loss: 3.3332 - val_accuracy: 0.2601
Epoch 121/128
 - 2s - loss: 1.0417 - accuracy: 0.5193 - val_loss: 2.6489 - val_accuracy: 0.2535
Epoch 122/128
 - 2s - loss: 1.0442 - accuracy: 0.5197 - val_loss: 3.2506 - val_accuracy: 0.2625
Epoch 123/128
 - 2s - loss: 1.0374 - accuracy: 0.5150 - val_loss: 3.0580 - val_accuracy: 0.2578
Epoch 124/128
 - 2s - loss: 1.0380 - accuracy: 0.5213 - val_loss: 2.8308 - val_accuracy: 0.2601
Epoch 125/128
 - 1s - loss: 1.0632 - accuracy: 0.5121 - val_loss: 2.9552 - val_accuracy: 0.2551
Epoch 126/128
 - 2s - loss: 1.0528 - accuracy: 0.5175 - val_loss: 2.9505 - val_accuracy: 0.2519
Epoch 127/128
 - 2s - loss: 1.0193 - accuracy: 0.5315 - val_loss: 3.0308 - val_accuracy: 0.2621
Epoch 128/128
 - 2s - loss: 0.9987 - accuracy: 0.5389 - val_loss: 3.0409 - val_accuracy: 0.2609

Fit: epochs= 128 , batch_size= 32 , verbose= 2 , shuffle= False , validation_split= 0.2 

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_2 (Dense)              (None, 500)               5500      
_________________________________________________________________
dense_3 (Dense)              (None, 300)               150300    
_________________________________________________________________
dense_4 (Dense)              (None, 200)               60200     
_________________________________________________________________
dense_5 (Dense)              (None, 100)               20100     
_________________________________________________________________
dense_6 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_7 (Dense)              (None, 20)                1020      
_________________________________________________________________
dense_8 (Dense)              (None, 4)                 84        
=================================================================
Total params: 242,364
Trainable params: 242,364
Non-trainable params: 0
_________________________________________________________________
None

Accuracy Train: 49.11%
Accuracy Test: 23.01%
Loss Train: 1.40
Loss Test: 29.00
Numero dati esaminati: 4280
True Positive 985
False Positive 3295
