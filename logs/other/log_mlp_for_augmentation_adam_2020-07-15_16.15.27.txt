Dataset used: ../../datasets/other/train_dataset_for_augmentation.csv 

   Unnamed: 0  Temperature  Sound  ...     Z2  Classification  Feedback
0           0           32      1  ... -15596             100     Happy
1           1           32      1  ... -15628             100     Happy
2           2           -1      1  ... -15612             100     Happy
3           3           -1     -1  ...     -1             100     Happy
4           4           32      1  ... -15720             100     Happy

[5 rows x 12 columns]

Objservations: 8560
Dataset used: ../../datasets/other/test_dataset_for_augmentation.csv 

   Temperature  Sound  Heartbeat   X1  ...    Y2     Z2  Classification  Feedback
0           35      1         64  844  ... -7000 -15764             250       Sad
1           35     -1         64  832  ...    -1     -1             250       Sad
2           35      1         64  768  ... -7000 -15800             250       Sad
3           -1      1         64   -1  ... -7168 -15892             250       Sad
4           35     -1         64  692  ...    -1     -1             250       Sad

[5 rows x 11 columns]

Objservations: 4280

Layers:

{'name': 'dense_1', 'trainable': True, 'batch_input_shape': (None, 10), 'dtype': 'float32', 'units': 10, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 500, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 300, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 100, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 50, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 20, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

Compile: loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']

Start computation...

Train on 6848 samples, validate on 1712 samples
Epoch 1/128
 - 1s - loss: 1.3851 - accuracy: 0.2691 - val_loss: 1.3865 - val_accuracy: 0.2699
Epoch 2/128
 - 1s - loss: 1.3807 - accuracy: 0.2883 - val_loss: 1.3859 - val_accuracy: 0.2704
Epoch 3/128
 - 1s - loss: 1.3783 - accuracy: 0.2913 - val_loss: 1.3877 - val_accuracy: 0.2710
Epoch 4/128
 - 1s - loss: 1.3777 - accuracy: 0.2951 - val_loss: 1.3878 - val_accuracy: 0.2804
Epoch 5/128
 - 1s - loss: 1.3764 - accuracy: 0.2979 - val_loss: 1.3896 - val_accuracy: 0.2810
Epoch 6/128
 - 1s - loss: 1.3756 - accuracy: 0.2998 - val_loss: 1.3906 - val_accuracy: 0.2880
Epoch 7/128
 - 1s - loss: 1.3746 - accuracy: 0.3062 - val_loss: 1.3920 - val_accuracy: 0.2874
Epoch 8/128
 - 1s - loss: 1.3765 - accuracy: 0.2886 - val_loss: 1.3915 - val_accuracy: 0.2815
Epoch 9/128
 - 1s - loss: 1.3748 - accuracy: 0.3011 - val_loss: 1.3896 - val_accuracy: 0.2856
Epoch 10/128
 - 1s - loss: 1.3731 - accuracy: 0.3001 - val_loss: 1.3901 - val_accuracy: 0.2938
Epoch 11/128
 - 1s - loss: 1.3738 - accuracy: 0.3005 - val_loss: 1.3936 - val_accuracy: 0.2973
Epoch 12/128
 - 1s - loss: 1.3708 - accuracy: 0.3026 - val_loss: 1.3975 - val_accuracy: 0.2810
Epoch 13/128
 - 1s - loss: 1.3690 - accuracy: 0.3218 - val_loss: 1.3926 - val_accuracy: 0.2921
Epoch 14/128
 - 1s - loss: 1.3661 - accuracy: 0.3255 - val_loss: 1.3937 - val_accuracy: 0.2932
Epoch 15/128
 - 1s - loss: 1.3621 - accuracy: 0.3213 - val_loss: 1.3990 - val_accuracy: 0.2880
Epoch 16/128
 - 1s - loss: 1.3600 - accuracy: 0.3240 - val_loss: 1.3946 - val_accuracy: 0.2839
Epoch 17/128
 - 1s - loss: 1.3565 - accuracy: 0.3281 - val_loss: 1.4013 - val_accuracy: 0.2833
Epoch 18/128
 - 1s - loss: 1.3604 - accuracy: 0.3278 - val_loss: 1.3920 - val_accuracy: 0.2815
Epoch 19/128
 - 1s - loss: 1.3579 - accuracy: 0.3224 - val_loss: 1.3989 - val_accuracy: 0.2839
Epoch 20/128
 - 1s - loss: 1.3524 - accuracy: 0.3300 - val_loss: 1.4040 - val_accuracy: 0.2938
Epoch 21/128
 - 1s - loss: 1.3480 - accuracy: 0.3293 - val_loss: 1.3987 - val_accuracy: 0.2734
Epoch 22/128
 - 1s - loss: 1.3395 - accuracy: 0.3369 - val_loss: 1.4064 - val_accuracy: 0.2769
Epoch 23/128
 - 1s - loss: 1.3408 - accuracy: 0.3376 - val_loss: 1.3976 - val_accuracy: 0.2745
Epoch 24/128
 - 1s - loss: 1.3382 - accuracy: 0.3522 - val_loss: 1.4049 - val_accuracy: 0.2827
Epoch 25/128
 - 1s - loss: 1.3251 - accuracy: 0.3570 - val_loss: 1.4014 - val_accuracy: 0.2862
Epoch 26/128
 - 1s - loss: 1.3178 - accuracy: 0.3709 - val_loss: 1.4051 - val_accuracy: 0.2821
Epoch 27/128
 - 1s - loss: 1.3127 - accuracy: 0.3636 - val_loss: 1.4046 - val_accuracy: 0.2815
Epoch 28/128
 - 1s - loss: 1.3105 - accuracy: 0.3678 - val_loss: 1.4117 - val_accuracy: 0.2827
Epoch 29/128
 - 1s - loss: 1.3005 - accuracy: 0.3807 - val_loss: 1.4179 - val_accuracy: 0.2763
Epoch 30/128
 - 1s - loss: 1.2997 - accuracy: 0.3719 - val_loss: 1.4170 - val_accuracy: 0.2757
Epoch 31/128
 - 1s - loss: 1.2975 - accuracy: 0.3732 - val_loss: 1.4099 - val_accuracy: 0.2722
Epoch 32/128
 - 1s - loss: 1.2954 - accuracy: 0.3801 - val_loss: 1.4124 - val_accuracy: 0.2856
Epoch 33/128
 - 1s - loss: 1.2829 - accuracy: 0.3915 - val_loss: 1.4164 - val_accuracy: 0.2856
Epoch 34/128
 - 1s - loss: 1.2805 - accuracy: 0.3851 - val_loss: 1.4155 - val_accuracy: 0.2804
Epoch 35/128
 - 1s - loss: 1.2717 - accuracy: 0.3934 - val_loss: 1.4140 - val_accuracy: 0.2850
Epoch 36/128
 - 1s - loss: 1.2704 - accuracy: 0.3995 - val_loss: 1.4269 - val_accuracy: 0.2804
Epoch 37/128
 - 1s - loss: 1.2779 - accuracy: 0.3908 - val_loss: 1.4171 - val_accuracy: 0.2739
Epoch 38/128
 - 1s - loss: 1.2806 - accuracy: 0.3900 - val_loss: 1.4163 - val_accuracy: 0.2938
Epoch 39/128
 - 1s - loss: 1.2728 - accuracy: 0.3912 - val_loss: 1.4196 - val_accuracy: 0.2821
Epoch 40/128
 - 1s - loss: 1.2809 - accuracy: 0.3864 - val_loss: 1.4128 - val_accuracy: 0.2804
Epoch 41/128
 - 1s - loss: 1.2609 - accuracy: 0.3995 - val_loss: 1.4129 - val_accuracy: 0.2868
Epoch 42/128
 - 1s - loss: 1.2495 - accuracy: 0.4046 - val_loss: 1.4188 - val_accuracy: 0.2886
Epoch 43/128
 - 1s - loss: 1.2518 - accuracy: 0.4060 - val_loss: 1.4288 - val_accuracy: 0.2926
Epoch 44/128
 - 1s - loss: 1.2401 - accuracy: 0.4121 - val_loss: 1.4349 - val_accuracy: 0.2775
Epoch 45/128
 - 1s - loss: 1.2397 - accuracy: 0.4223 - val_loss: 1.4321 - val_accuracy: 0.2821
Epoch 46/128
 - 1s - loss: 1.2550 - accuracy: 0.4147 - val_loss: 1.4087 - val_accuracy: 0.2810
Epoch 47/128
 - 1s - loss: 1.2463 - accuracy: 0.4138 - val_loss: 1.4501 - val_accuracy: 0.2728
Epoch 48/128
 - 1s - loss: 1.2503 - accuracy: 0.4090 - val_loss: 1.4414 - val_accuracy: 0.2681
Epoch 49/128
 - 1s - loss: 1.2307 - accuracy: 0.4248 - val_loss: 1.4157 - val_accuracy: 0.3049
Epoch 50/128
 - 1s - loss: 1.2502 - accuracy: 0.4122 - val_loss: 1.4461 - val_accuracy: 0.2798
Epoch 51/128
 - 1s - loss: 1.2347 - accuracy: 0.4220 - val_loss: 1.4527 - val_accuracy: 0.2722
Epoch 52/128
 - 1s - loss: 1.2212 - accuracy: 0.4315 - val_loss: 1.4412 - val_accuracy: 0.2815
Epoch 53/128
 - 1s - loss: 1.2093 - accuracy: 0.4346 - val_loss: 1.4644 - val_accuracy: 0.2973
Epoch 54/128
 - 1s - loss: 1.1997 - accuracy: 0.4357 - val_loss: 1.4678 - val_accuracy: 0.2909
Epoch 55/128
 - 1s - loss: 1.2021 - accuracy: 0.4413 - val_loss: 1.4949 - val_accuracy: 0.2780
Epoch 56/128
 - 1s - loss: 1.2070 - accuracy: 0.4416 - val_loss: 1.4552 - val_accuracy: 0.3172
Epoch 57/128
 - 1s - loss: 1.2009 - accuracy: 0.4404 - val_loss: 1.4772 - val_accuracy: 0.2926
Epoch 58/128
 - 1s - loss: 1.1869 - accuracy: 0.4502 - val_loss: 1.4792 - val_accuracy: 0.2996
Epoch 59/128
 - 1s - loss: 1.2047 - accuracy: 0.4365 - val_loss: 1.4920 - val_accuracy: 0.3172
Epoch 60/128
 - 1s - loss: 1.1964 - accuracy: 0.4483 - val_loss: 1.5270 - val_accuracy: 0.3037
Epoch 61/128
 - 1s - loss: 1.1919 - accuracy: 0.4390 - val_loss: 1.5051 - val_accuracy: 0.3043
Epoch 62/128
 - 1s - loss: 1.1772 - accuracy: 0.4520 - val_loss: 1.5325 - val_accuracy: 0.2839
Epoch 63/128
 - 1s - loss: 1.1689 - accuracy: 0.4550 - val_loss: 1.5129 - val_accuracy: 0.2786
Epoch 64/128
 - 1s - loss: 1.1676 - accuracy: 0.4565 - val_loss: 1.5222 - val_accuracy: 0.2973
Epoch 65/128
 - 1s - loss: 1.1552 - accuracy: 0.4563 - val_loss: 1.6059 - val_accuracy: 0.2961
Epoch 66/128
 - 1s - loss: 1.1693 - accuracy: 0.4536 - val_loss: 1.6163 - val_accuracy: 0.3084
Epoch 67/128
 - 1s - loss: 1.1520 - accuracy: 0.4644 - val_loss: 1.6321 - val_accuracy: 0.2798
Epoch 68/128
 - 1s - loss: 1.1281 - accuracy: 0.4734 - val_loss: 1.6546 - val_accuracy: 0.3032
Epoch 69/128
 - 1s - loss: 1.1310 - accuracy: 0.4692 - val_loss: 1.6929 - val_accuracy: 0.2991
Epoch 70/128
 - 1s - loss: 1.1345 - accuracy: 0.4724 - val_loss: 1.6463 - val_accuracy: 0.2921
Epoch 71/128
 - 1s - loss: 1.1339 - accuracy: 0.4706 - val_loss: 1.6482 - val_accuracy: 0.3072
Epoch 72/128
 - 1s - loss: 1.1112 - accuracy: 0.4791 - val_loss: 1.6872 - val_accuracy: 0.3213
Epoch 73/128
 - 1s - loss: 1.1077 - accuracy: 0.4907 - val_loss: 1.6507 - val_accuracy: 0.3049
Epoch 74/128
 - 1s - loss: 1.1052 - accuracy: 0.4879 - val_loss: 1.6245 - val_accuracy: 0.3061
Epoch 75/128
 - 1s - loss: 1.1202 - accuracy: 0.4775 - val_loss: 1.7195 - val_accuracy: 0.3055
Epoch 76/128
 - 1s - loss: 1.1455 - accuracy: 0.4771 - val_loss: 1.8315 - val_accuracy: 0.3195
Epoch 77/128
 - 1s - loss: 1.0949 - accuracy: 0.4933 - val_loss: 1.7005 - val_accuracy: 0.3166
Epoch 78/128
 - 1s - loss: 1.1116 - accuracy: 0.4905 - val_loss: 1.7602 - val_accuracy: 0.2961
Epoch 79/128
 - 1s - loss: 1.0888 - accuracy: 0.4969 - val_loss: 1.7917 - val_accuracy: 0.2956
Epoch 80/128
 - 1s - loss: 1.0965 - accuracy: 0.4927 - val_loss: 1.8526 - val_accuracy: 0.3026
Epoch 81/128
 - 1s - loss: 1.0921 - accuracy: 0.4937 - val_loss: 1.8180 - val_accuracy: 0.3055
Epoch 82/128
 - 1s - loss: 1.0750 - accuracy: 0.5029 - val_loss: 1.7953 - val_accuracy: 0.2944
Epoch 83/128
 - 1s - loss: 1.0640 - accuracy: 0.5072 - val_loss: 1.9305 - val_accuracy: 0.2915
Epoch 84/128
 - 1s - loss: 1.0758 - accuracy: 0.4974 - val_loss: 1.8898 - val_accuracy: 0.2880
Epoch 85/128
 - 1s - loss: 1.0894 - accuracy: 0.4990 - val_loss: 1.8003 - val_accuracy: 0.2886
Epoch 86/128
 - 1s - loss: 1.0640 - accuracy: 0.5064 - val_loss: 1.7654 - val_accuracy: 0.2868
Epoch 87/128
 - 1s - loss: 1.0940 - accuracy: 0.4947 - val_loss: 1.8924 - val_accuracy: 0.2961
Epoch 88/128
 - 1s - loss: 1.0565 - accuracy: 0.5121 - val_loss: 1.9454 - val_accuracy: 0.2956
Epoch 89/128
 - 1s - loss: 1.0590 - accuracy: 0.5069 - val_loss: 1.9517 - val_accuracy: 0.2839
Epoch 90/128
 - 1s - loss: 1.0756 - accuracy: 0.5086 - val_loss: 1.9482 - val_accuracy: 0.2886
Epoch 91/128
 - 1s - loss: 1.0447 - accuracy: 0.5206 - val_loss: 2.0012 - val_accuracy: 0.2979
Epoch 92/128
 - 1s - loss: 1.0536 - accuracy: 0.5143 - val_loss: 1.9571 - val_accuracy: 0.2944
Epoch 93/128
 - 1s - loss: 1.0494 - accuracy: 0.5190 - val_loss: 2.1353 - val_accuracy: 0.2932
Epoch 94/128
 - 1s - loss: 1.0301 - accuracy: 0.5221 - val_loss: 2.1147 - val_accuracy: 0.2833
Epoch 95/128
 - 1s - loss: 1.0217 - accuracy: 0.5273 - val_loss: 2.1492 - val_accuracy: 0.2944
Epoch 96/128
 - 1s - loss: 1.0193 - accuracy: 0.5333 - val_loss: 2.1763 - val_accuracy: 0.2845
Epoch 97/128
 - 1s - loss: 1.0288 - accuracy: 0.5308 - val_loss: 1.9917 - val_accuracy: 0.2868
Epoch 98/128
 - 1s - loss: 1.0155 - accuracy: 0.5333 - val_loss: 1.9689 - val_accuracy: 0.2804
Epoch 99/128
 - 1s - loss: 0.9943 - accuracy: 0.5435 - val_loss: 2.1454 - val_accuracy: 0.2921
Epoch 100/128
 - 1s - loss: 1.0186 - accuracy: 0.5272 - val_loss: 2.1964 - val_accuracy: 0.2891
Epoch 101/128
 - 1s - loss: 1.0334 - accuracy: 0.5237 - val_loss: 2.0526 - val_accuracy: 0.2798
Epoch 102/128
 - 1s - loss: 1.0063 - accuracy: 0.5402 - val_loss: 2.1784 - val_accuracy: 0.2839
Epoch 103/128
 - 1s - loss: 1.0093 - accuracy: 0.5273 - val_loss: 2.1748 - val_accuracy: 0.2868
Epoch 104/128
 - 1s - loss: 0.9879 - accuracy: 0.5445 - val_loss: 2.1202 - val_accuracy: 0.2827
Epoch 105/128
 - 1s - loss: 1.0127 - accuracy: 0.5311 - val_loss: 2.4610 - val_accuracy: 0.2915
Epoch 106/128
 - 1s - loss: 0.9762 - accuracy: 0.5466 - val_loss: 2.3856 - val_accuracy: 0.2874
Epoch 107/128
 - 1s - loss: 0.9600 - accuracy: 0.5600 - val_loss: 2.4072 - val_accuracy: 0.2897
Epoch 108/128
 - 1s - loss: 0.9735 - accuracy: 0.5511 - val_loss: 2.4569 - val_accuracy: 0.2827
Epoch 109/128
 - 1s - loss: 1.0325 - accuracy: 0.5298 - val_loss: 2.4291 - val_accuracy: 0.2856
Epoch 110/128
 - 1s - loss: 0.9915 - accuracy: 0.5428 - val_loss: 2.5336 - val_accuracy: 0.2862
Epoch 111/128
 - 1s - loss: 0.9695 - accuracy: 0.5612 - val_loss: 2.3050 - val_accuracy: 0.2886
Epoch 112/128
 - 1s - loss: 1.0297 - accuracy: 0.5364 - val_loss: 2.5522 - val_accuracy: 0.2839
Epoch 113/128
 - 1s - loss: 1.0204 - accuracy: 0.5350 - val_loss: 2.2602 - val_accuracy: 0.2903
Epoch 114/128
 - 1s - loss: 0.9935 - accuracy: 0.5368 - val_loss: 2.5681 - val_accuracy: 0.2845
Epoch 115/128
 - 1s - loss: 0.9615 - accuracy: 0.5565 - val_loss: 2.4368 - val_accuracy: 0.2921
Epoch 116/128
 - 1s - loss: 0.9596 - accuracy: 0.5539 - val_loss: 2.3354 - val_accuracy: 0.2833
Epoch 117/128
 - 1s - loss: 0.9509 - accuracy: 0.5594 - val_loss: 2.3455 - val_accuracy: 0.2833
Epoch 118/128
 - 1s - loss: 1.0035 - accuracy: 0.5459 - val_loss: 2.4064 - val_accuracy: 0.2780
Epoch 119/128
 - 1s - loss: 0.9533 - accuracy: 0.5667 - val_loss: 2.5419 - val_accuracy: 0.2739
Epoch 120/128
 - 1s - loss: 0.9258 - accuracy: 0.5774 - val_loss: 2.5391 - val_accuracy: 0.2763
Epoch 121/128
 - 1s - loss: 0.9235 - accuracy: 0.5799 - val_loss: 2.5986 - val_accuracy: 0.2909
Epoch 122/128
 - 1s - loss: 0.9538 - accuracy: 0.5607 - val_loss: 2.5856 - val_accuracy: 0.2821
Epoch 123/128
 - 1s - loss: 0.9665 - accuracy: 0.5572 - val_loss: 2.4755 - val_accuracy: 0.2722
Epoch 124/128
 - 2s - loss: 0.9821 - accuracy: 0.5533 - val_loss: 2.4845 - val_accuracy: 0.2833
Epoch 125/128
 - 1s - loss: 0.9385 - accuracy: 0.5686 - val_loss: 2.6835 - val_accuracy: 0.2891
Epoch 126/128
 - 1s - loss: 0.8993 - accuracy: 0.5818 - val_loss: 2.7429 - val_accuracy: 0.2827
Epoch 127/128
 - 1s - loss: 0.9247 - accuracy: 0.5768 - val_loss: 2.7880 - val_accuracy: 0.2880
Epoch 128/128
 - 1s - loss: 0.8905 - accuracy: 0.5879 - val_loss: 3.0021 - val_accuracy: 0.2728

Fit: epochs= 128 , batch_size= 32 , verbose= 2 , shuffle= False , validation_split= 0.2 

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_2 (Dense)              (None, 500)               5500      
_________________________________________________________________
dense_3 (Dense)              (None, 300)               150300    
_________________________________________________________________
dense_4 (Dense)              (None, 200)               60200     
_________________________________________________________________
dense_5 (Dense)              (None, 100)               20100     
_________________________________________________________________
dense_6 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_7 (Dense)              (None, 20)                1020      
_________________________________________________________________
dense_8 (Dense)              (None, 4)                 84        
=================================================================
Total params: 242,364
Trainable params: 242,364
Non-trainable params: 0
_________________________________________________________________
None

Accuracy Train: 52.85%
Accuracy Test: 24.79%
Loss Train: 1.33
Loss Test: 8.64
Numero dati esaminati: 4280
True Positive 1061
False Positive 3219
