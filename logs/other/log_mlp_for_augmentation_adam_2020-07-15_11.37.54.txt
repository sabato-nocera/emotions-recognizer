Dataset used: ../../datasets/other/train_dataset_for_augmentation.csv 

   Unnamed: 0  Temperature  Sound  ...     Z2  Classification  Feedback
0           0         32.0      1  ... -15596             100     Happy
1           1         32.0      1  ... -15628             100     Happy
2           2         -1.0      1  ... -15612             100     Happy
3           3         -1.0     -1  ...     -1             100     Happy
4           4         32.0      1  ... -15720             100     Happy

[5 rows x 12 columns]

Objservations: 12840
Dataset used: ../../datasets/other/test_dataset_for_augmentation.csv 

   Temperature  Sound  Heartbeat   X1  ...    Y2     Z2  Classification  Feedback
0           35      1         64  844  ... -7000 -15764             250       Sad
1           35     -1         64  832  ...    -1     -1             250       Sad
2           35      1         64  768  ... -7000 -15800             250       Sad
3           -1      1         64   -1  ... -7168 -15892             250       Sad
4           35     -1         64  692  ...    -1     -1             250       Sad

[5 rows x 11 columns]

Objservations: 4280

Layers:

{'name': 'dense_1', 'trainable': True, 'batch_input_shape': (None, 10), 'dtype': 'float32', 'units': 10, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 500, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 300, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 100, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 50, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 20, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

Compile: loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']

Start computation...

Train on 10272 samples, validate on 2568 samples
Epoch 1/128
 - 2s - loss: 1.3839 - accuracy: 0.2817 - val_loss: 1.3859 - val_accuracy: 0.2718
Epoch 2/128
 - 1s - loss: 1.3810 - accuracy: 0.2922 - val_loss: 1.3866 - val_accuracy: 0.2765
Epoch 3/128
 - 1s - loss: 1.3792 - accuracy: 0.2965 - val_loss: 1.3875 - val_accuracy: 0.2695
Epoch 4/128
 - 1s - loss: 1.3780 - accuracy: 0.3026 - val_loss: 1.3860 - val_accuracy: 0.2765
Epoch 5/128
 - 1s - loss: 1.3782 - accuracy: 0.3028 - val_loss: 1.3881 - val_accuracy: 0.2687
Epoch 6/128
 - 2s - loss: 1.3768 - accuracy: 0.3049 - val_loss: 1.3913 - val_accuracy: 0.2574
Epoch 7/128
 - 1s - loss: 1.3771 - accuracy: 0.2966 - val_loss: 1.3915 - val_accuracy: 0.2636
Epoch 8/128
 - 2s - loss: 1.3769 - accuracy: 0.3012 - val_loss: 1.3907 - val_accuracy: 0.2492
Epoch 9/128
 - 2s - loss: 1.3787 - accuracy: 0.2910 - val_loss: 1.3899 - val_accuracy: 0.2442
Epoch 10/128
 - 2s - loss: 1.3761 - accuracy: 0.3058 - val_loss: 1.3938 - val_accuracy: 0.2644
Epoch 11/128
 - 2s - loss: 1.3755 - accuracy: 0.3104 - val_loss: 1.3869 - val_accuracy: 0.2570
Epoch 12/128
 - 2s - loss: 1.3763 - accuracy: 0.2958 - val_loss: 1.3875 - val_accuracy: 0.2640
Epoch 13/128
 - 2s - loss: 1.3746 - accuracy: 0.2934 - val_loss: 1.3865 - val_accuracy: 0.2601
Epoch 14/128
 - 2s - loss: 1.3724 - accuracy: 0.3102 - val_loss: 1.3895 - val_accuracy: 0.2629
Epoch 15/128
 - 2s - loss: 1.3723 - accuracy: 0.3184 - val_loss: 1.3885 - val_accuracy: 0.2644
Epoch 16/128
 - 2s - loss: 1.3658 - accuracy: 0.3249 - val_loss: 1.3928 - val_accuracy: 0.2617
Epoch 17/128
 - 2s - loss: 1.3665 - accuracy: 0.3132 - val_loss: 1.3872 - val_accuracy: 0.2702
Epoch 18/128
 - 2s - loss: 1.3593 - accuracy: 0.3230 - val_loss: 1.3862 - val_accuracy: 0.2590
Epoch 19/128
 - 3s - loss: 1.3556 - accuracy: 0.3230 - val_loss: 1.3835 - val_accuracy: 0.2652
Epoch 20/128
 - 3s - loss: 1.3604 - accuracy: 0.3299 - val_loss: 1.3863 - val_accuracy: 0.2956
Epoch 21/128
 - 3s - loss: 1.3658 - accuracy: 0.3247 - val_loss: 1.3866 - val_accuracy: 0.3033
Epoch 22/128
 - 2s - loss: 1.3574 - accuracy: 0.3143 - val_loss: 1.3938 - val_accuracy: 0.2679
Epoch 23/128
 - 2s - loss: 1.3475 - accuracy: 0.3317 - val_loss: 1.3846 - val_accuracy: 0.2835
Epoch 24/128
 - 2s - loss: 1.3474 - accuracy: 0.3290 - val_loss: 1.3763 - val_accuracy: 0.3072
Epoch 25/128
 - 2s - loss: 1.3420 - accuracy: 0.3430 - val_loss: 1.3919 - val_accuracy: 0.2726
Epoch 26/128
 - 2s - loss: 1.3300 - accuracy: 0.3486 - val_loss: 1.3690 - val_accuracy: 0.3388
Epoch 27/128
 - 2s - loss: 1.3256 - accuracy: 0.3507 - val_loss: 1.3714 - val_accuracy: 0.3154
Epoch 28/128
 - 2s - loss: 1.3230 - accuracy: 0.3499 - val_loss: 1.3607 - val_accuracy: 0.3224
Epoch 29/128
 - 2s - loss: 1.2979 - accuracy: 0.3792 - val_loss: 1.3660 - val_accuracy: 0.3181
Epoch 30/128
 - 2s - loss: 1.2923 - accuracy: 0.3747 - val_loss: 1.3677 - val_accuracy: 0.3178
Epoch 31/128
 - 2s - loss: 1.2941 - accuracy: 0.3830 - val_loss: 1.3510 - val_accuracy: 0.3275
Epoch 32/128
 - 2s - loss: 1.2875 - accuracy: 0.3706 - val_loss: 1.3593 - val_accuracy: 0.3435
Epoch 33/128
 - 2s - loss: 1.2770 - accuracy: 0.3946 - val_loss: 1.3434 - val_accuracy: 0.3466
Epoch 34/128
 - 2s - loss: 1.2649 - accuracy: 0.3994 - val_loss: 1.3476 - val_accuracy: 0.3485
Epoch 35/128
 - 2s - loss: 1.2537 - accuracy: 0.3970 - val_loss: 1.3456 - val_accuracy: 0.3610
Epoch 36/128
 - 2s - loss: 1.2713 - accuracy: 0.4002 - val_loss: 1.3741 - val_accuracy: 0.3080
Epoch 37/128
 - 2s - loss: 1.2512 - accuracy: 0.4211 - val_loss: 1.3816 - val_accuracy: 0.3041
Epoch 38/128
 - 2s - loss: 1.2215 - accuracy: 0.4302 - val_loss: 1.3325 - val_accuracy: 0.3664
Epoch 39/128
 - 2s - loss: 1.2414 - accuracy: 0.4154 - val_loss: 1.3315 - val_accuracy: 0.3688
Epoch 40/128
 - 2s - loss: 1.2213 - accuracy: 0.4272 - val_loss: 1.3444 - val_accuracy: 0.3664
Epoch 41/128
 - 2s - loss: 1.1958 - accuracy: 0.4395 - val_loss: 1.3228 - val_accuracy: 0.3746
Epoch 42/128
 - 2s - loss: 1.1931 - accuracy: 0.4453 - val_loss: 1.2881 - val_accuracy: 0.4093
Epoch 43/128
 - 2s - loss: 1.1925 - accuracy: 0.4413 - val_loss: 1.2861 - val_accuracy: 0.4003
Epoch 44/128
 - 2s - loss: 1.1757 - accuracy: 0.4489 - val_loss: 1.2928 - val_accuracy: 0.4136
Epoch 45/128
 - 2s - loss: 1.1783 - accuracy: 0.4561 - val_loss: 1.3193 - val_accuracy: 0.3824
Epoch 46/128
 - 2s - loss: 1.1945 - accuracy: 0.4534 - val_loss: 1.2945 - val_accuracy: 0.4085
Epoch 47/128
 - 2s - loss: 1.1694 - accuracy: 0.4567 - val_loss: 1.3025 - val_accuracy: 0.4019
Epoch 48/128
 - 2s - loss: 1.1528 - accuracy: 0.4684 - val_loss: 1.2911 - val_accuracy: 0.4112
Epoch 49/128
 - 2s - loss: 1.1887 - accuracy: 0.4504 - val_loss: 1.3141 - val_accuracy: 0.4007
Epoch 50/128
 - 2s - loss: 1.1665 - accuracy: 0.4599 - val_loss: 1.3004 - val_accuracy: 0.4054
Epoch 51/128
 - 2s - loss: 1.1470 - accuracy: 0.4676 - val_loss: 1.2924 - val_accuracy: 0.4085
Epoch 52/128
 - 2s - loss: 1.1340 - accuracy: 0.4746 - val_loss: 1.3151 - val_accuracy: 0.4143
Epoch 53/128
 - 2s - loss: 1.1375 - accuracy: 0.4750 - val_loss: 1.3902 - val_accuracy: 0.3956
Epoch 54/128
 - 2s - loss: 1.1376 - accuracy: 0.4733 - val_loss: 1.3481 - val_accuracy: 0.3964
Epoch 55/128
 - 2s - loss: 1.1565 - accuracy: 0.4651 - val_loss: 1.2949 - val_accuracy: 0.4237
Epoch 56/128
 - 2s - loss: 1.1579 - accuracy: 0.4726 - val_loss: 1.3060 - val_accuracy: 0.4077
Epoch 57/128
 - 2s - loss: 1.1499 - accuracy: 0.4718 - val_loss: 1.3430 - val_accuracy: 0.3879
Epoch 58/128
 - 2s - loss: 1.1519 - accuracy: 0.4743 - val_loss: 1.2568 - val_accuracy: 0.4190
Epoch 59/128
 - 2s - loss: 1.1165 - accuracy: 0.4845 - val_loss: 1.2827 - val_accuracy: 0.4186
Epoch 60/128
 - 2s - loss: 1.0743 - accuracy: 0.5092 - val_loss: 1.2578 - val_accuracy: 0.4120
Epoch 61/128
 - 2s - loss: 1.0965 - accuracy: 0.5002 - val_loss: 1.2700 - val_accuracy: 0.4100
Epoch 62/128
 - 2s - loss: 1.1081 - accuracy: 0.4909 - val_loss: 1.2937 - val_accuracy: 0.4225
Epoch 63/128
 - 2s - loss: 1.0891 - accuracy: 0.5006 - val_loss: 1.3013 - val_accuracy: 0.4116
Epoch 64/128
 - 2s - loss: 1.0715 - accuracy: 0.5148 - val_loss: 1.3373 - val_accuracy: 0.4089
Epoch 65/128
 - 2s - loss: 1.1118 - accuracy: 0.4936 - val_loss: 1.3678 - val_accuracy: 0.3882
Epoch 66/128
 - 2s - loss: 1.1024 - accuracy: 0.5043 - val_loss: 1.3501 - val_accuracy: 0.4330
Epoch 67/128
 - 2s - loss: 1.0923 - accuracy: 0.5025 - val_loss: 1.3512 - val_accuracy: 0.4116
Epoch 68/128
 - 2s - loss: 1.0796 - accuracy: 0.5138 - val_loss: 1.3081 - val_accuracy: 0.4272
Epoch 69/128
 - 2s - loss: 1.0945 - accuracy: 0.5045 - val_loss: 1.3384 - val_accuracy: 0.4256
Epoch 70/128
 - 2s - loss: 1.0561 - accuracy: 0.5202 - val_loss: 1.3319 - val_accuracy: 0.4303
Epoch 71/128
 - 2s - loss: 1.0369 - accuracy: 0.5356 - val_loss: 1.2068 - val_accuracy: 0.4700
Epoch 72/128
 - 2s - loss: 1.0439 - accuracy: 0.5240 - val_loss: 1.2917 - val_accuracy: 0.4428
Epoch 73/128
 - 2s - loss: 1.1029 - accuracy: 0.4997 - val_loss: 1.2313 - val_accuracy: 0.4424
Epoch 74/128
 - 2s - loss: 1.0755 - accuracy: 0.5152 - val_loss: 1.2658 - val_accuracy: 0.4311
Epoch 75/128
 - 2s - loss: 1.0352 - accuracy: 0.5356 - val_loss: 1.2196 - val_accuracy: 0.4560
Epoch 76/128
 - 2s - loss: 1.0467 - accuracy: 0.5223 - val_loss: 1.2762 - val_accuracy: 0.4233
Epoch 77/128
 - 2s - loss: 1.0486 - accuracy: 0.5239 - val_loss: 1.2631 - val_accuracy: 0.4494
Epoch 78/128
 - 2s - loss: 1.0362 - accuracy: 0.5311 - val_loss: 1.2108 - val_accuracy: 0.4568
Epoch 79/128
 - 2s - loss: 1.0073 - accuracy: 0.5402 - val_loss: 1.2583 - val_accuracy: 0.4299
Epoch 80/128
 - 2s - loss: 1.0131 - accuracy: 0.5344 - val_loss: 1.2346 - val_accuracy: 0.4611
Epoch 81/128
 - 2s - loss: 1.0304 - accuracy: 0.5332 - val_loss: 1.2658 - val_accuracy: 0.4373
Epoch 82/128
 - 2s - loss: 1.0583 - accuracy: 0.5134 - val_loss: 1.2353 - val_accuracy: 0.4568
Epoch 83/128
 - 2s - loss: 1.0391 - accuracy: 0.5221 - val_loss: 1.1815 - val_accuracy: 0.4673
Epoch 84/128
 - 2s - loss: 1.0154 - accuracy: 0.5360 - val_loss: 1.2490 - val_accuracy: 0.4474
Epoch 85/128
 - 2s - loss: 1.0149 - accuracy: 0.5393 - val_loss: 1.2225 - val_accuracy: 0.4533
Epoch 86/128
 - 2s - loss: 1.0245 - accuracy: 0.5300 - val_loss: 1.3211 - val_accuracy: 0.4206
Epoch 87/128
 - 2s - loss: 1.0266 - accuracy: 0.5351 - val_loss: 1.2198 - val_accuracy: 0.4439
Epoch 88/128
 - 2s - loss: 0.9917 - accuracy: 0.5430 - val_loss: 1.1871 - val_accuracy: 0.4642
Epoch 89/128
 - 2s - loss: 1.0157 - accuracy: 0.5394 - val_loss: 1.2608 - val_accuracy: 0.4346
Epoch 90/128
 - 2s - loss: 1.0087 - accuracy: 0.5381 - val_loss: 1.2303 - val_accuracy: 0.4455
Epoch 91/128
 - 2s - loss: 1.0041 - accuracy: 0.5391 - val_loss: 1.1709 - val_accuracy: 0.4579
Epoch 92/128
 - 2s - loss: 0.9950 - accuracy: 0.5401 - val_loss: 1.1546 - val_accuracy: 0.4630
Epoch 93/128
 - 2s - loss: 0.9743 - accuracy: 0.5481 - val_loss: 1.2137 - val_accuracy: 0.4529
Epoch 94/128
 - 2s - loss: 0.9972 - accuracy: 0.5452 - val_loss: 1.1694 - val_accuracy: 0.4599
Epoch 95/128
 - 2s - loss: 0.9933 - accuracy: 0.5438 - val_loss: 1.1340 - val_accuracy: 0.4762
Epoch 96/128
 - 2s - loss: 0.9749 - accuracy: 0.5546 - val_loss: 1.1992 - val_accuracy: 0.4502
Epoch 97/128
 - 2s - loss: 0.9835 - accuracy: 0.5506 - val_loss: 1.1690 - val_accuracy: 0.4743
Epoch 98/128
 - 2s - loss: 1.0171 - accuracy: 0.5385 - val_loss: 1.1717 - val_accuracy: 0.4591
Epoch 99/128
 - 2s - loss: 1.0524 - accuracy: 0.5194 - val_loss: 1.2715 - val_accuracy: 0.4237
Epoch 100/128
 - 2s - loss: 1.0088 - accuracy: 0.5400 - val_loss: 1.1409 - val_accuracy: 0.4751
Epoch 101/128
 - 2s - loss: 0.9863 - accuracy: 0.5542 - val_loss: 1.1537 - val_accuracy: 0.4634
Epoch 102/128
 - 2s - loss: 0.9848 - accuracy: 0.5546 - val_loss: 1.1092 - val_accuracy: 0.4755
Epoch 103/128
 - 2s - loss: 0.9896 - accuracy: 0.5539 - val_loss: 1.2181 - val_accuracy: 0.4537
Epoch 104/128
 - 2s - loss: 0.9606 - accuracy: 0.5639 - val_loss: 1.1476 - val_accuracy: 0.4688
Epoch 105/128
 - 2s - loss: 0.9826 - accuracy: 0.5536 - val_loss: 1.1375 - val_accuracy: 0.4813
Epoch 106/128
 - 2s - loss: 0.9968 - accuracy: 0.5524 - val_loss: 1.1506 - val_accuracy: 0.4778
Epoch 107/128
 - 2s - loss: 0.9731 - accuracy: 0.5553 - val_loss: 1.1041 - val_accuracy: 0.4930
Epoch 108/128
 - 2s - loss: 0.9368 - accuracy: 0.5723 - val_loss: 1.1379 - val_accuracy: 0.4844
Epoch 109/128
 - 2s - loss: 0.9413 - accuracy: 0.5740 - val_loss: 1.1187 - val_accuracy: 0.4899
Epoch 110/128
 - 2s - loss: 0.9571 - accuracy: 0.5633 - val_loss: 1.1296 - val_accuracy: 0.4887
Epoch 111/128
 - 2s - loss: 0.9625 - accuracy: 0.5560 - val_loss: 1.1633 - val_accuracy: 0.4665
Epoch 112/128
 - 2s - loss: 0.9746 - accuracy: 0.5567 - val_loss: 1.0813 - val_accuracy: 0.4938
Epoch 113/128
 - 2s - loss: 0.9502 - accuracy: 0.5703 - val_loss: 1.2074 - val_accuracy: 0.4564
Epoch 114/128
 - 2s - loss: 1.0297 - accuracy: 0.5490 - val_loss: 1.2010 - val_accuracy: 0.4665
Epoch 115/128
 - 2s - loss: 0.9534 - accuracy: 0.5650 - val_loss: 1.1116 - val_accuracy: 0.4969
Epoch 116/128
 - 2s - loss: 0.9210 - accuracy: 0.5819 - val_loss: 1.1022 - val_accuracy: 0.5012
Epoch 117/128
 - 2s - loss: 0.9181 - accuracy: 0.5796 - val_loss: 1.1155 - val_accuracy: 0.4996
Epoch 118/128
 - 2s - loss: 0.9126 - accuracy: 0.5851 - val_loss: 1.2531 - val_accuracy: 0.4529
Epoch 119/128
 - 2s - loss: 0.9324 - accuracy: 0.5751 - val_loss: 1.1167 - val_accuracy: 0.4836
Epoch 120/128
 - 2s - loss: 0.9678 - accuracy: 0.5566 - val_loss: 1.1914 - val_accuracy: 0.4603
Epoch 121/128
 - 2s - loss: 0.9388 - accuracy: 0.5678 - val_loss: 1.1454 - val_accuracy: 0.4961
Epoch 122/128
 - 2s - loss: 0.9146 - accuracy: 0.5827 - val_loss: 1.1286 - val_accuracy: 0.4942
Epoch 123/128
 - 2s - loss: 0.9569 - accuracy: 0.5598 - val_loss: 1.0948 - val_accuracy: 0.5070
Epoch 124/128
 - 2s - loss: 1.0011 - accuracy: 0.5570 - val_loss: 1.1110 - val_accuracy: 0.4833
Epoch 125/128
 - 2s - loss: 0.9473 - accuracy: 0.5685 - val_loss: 1.1562 - val_accuracy: 0.4938
Epoch 126/128
 - 2s - loss: 0.9176 - accuracy: 0.5833 - val_loss: 1.4736 - val_accuracy: 0.4361
Epoch 127/128
 - 2s - loss: 0.9184 - accuracy: 0.5790 - val_loss: 1.1455 - val_accuracy: 0.4829
Epoch 128/128
 - 2s - loss: 0.8936 - accuracy: 0.5934 - val_loss: 1.4025 - val_accuracy: 0.4502

Fit: epochs= 128 , batch_size= 32 , verbose= 2 , shuffle= False , validation_split= 0.2 

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_2 (Dense)              (None, 500)               5500      
_________________________________________________________________
dense_3 (Dense)              (None, 300)               150300    
_________________________________________________________________
dense_4 (Dense)              (None, 200)               60200     
_________________________________________________________________
dense_5 (Dense)              (None, 100)               20100     
_________________________________________________________________
dense_6 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_7 (Dense)              (None, 20)                1020      
_________________________________________________________________
dense_8 (Dense)              (None, 4)                 84        
=================================================================
Total params: 242,364
Trainable params: 242,364
Non-trainable params: 0
_________________________________________________________________
None

Accuracy Train: 54.34%
Accuracy Test: 32.15%
Loss Train: 1.06
Loss Test: 5.49
Numero dati esaminati: 4280
True Positive 1376
False Positive 2904
