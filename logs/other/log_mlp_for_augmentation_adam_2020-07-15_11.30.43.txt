Dataset used: ../../datasets/other/train_dataset_for_augmentation.csv 

   Unnamed: 0  Temperature  Sound  ...     Z2  Classification  Feedback
0           0         32.0      1  ... -15596             100     Happy
1           1         32.0      1  ... -15628             100     Happy
2           2         -1.0      1  ... -15612             100     Happy
3           3         -1.0     -1  ...     -1             100     Happy
4           4         32.0      1  ... -15720             100     Happy

[5 rows x 12 columns]

Objservations: 25680
Dataset used: ../../datasets/other/test_dataset_for_augmentation.csv 

   Temperature  Sound  Heartbeat   X1  ...    Y2     Z2  Classification  Feedback
0           35      1         64  844  ... -7000 -15764             250       Sad
1           35     -1         64  832  ...    -1     -1             250       Sad
2           35      1         64  768  ... -7000 -15800             250       Sad
3           -1      1         64   -1  ... -7168 -15892             250       Sad
4           35     -1         64  692  ...    -1     -1             250       Sad

[5 rows x 11 columns]

Objservations: 4280

Layers:

{'name': 'dense_1', 'trainable': True, 'batch_input_shape': (None, 10), 'dtype': 'float32', 'units': 10, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 500, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 300, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 100, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 50, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 20, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

{'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 4, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} 

Compile: loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']

Start computation...

Train on 20544 samples, validate on 5136 samples
Epoch 1/128
 - 3s - loss: 1.3889 - accuracy: 0.2466 - val_loss: 1.3847 - val_accuracy: 0.2991
Epoch 2/128
 - 2s - loss: 1.3860 - accuracy: 0.2779 - val_loss: 1.3836 - val_accuracy: 0.2845
Epoch 3/128
 - 2s - loss: 1.3848 - accuracy: 0.2782 - val_loss: 1.3839 - val_accuracy: 0.2845
Epoch 4/128
 - 2s - loss: 1.3840 - accuracy: 0.2840 - val_loss: 1.3835 - val_accuracy: 0.2739
Epoch 5/128
 - 2s - loss: 1.3831 - accuracy: 0.2886 - val_loss: 1.3828 - val_accuracy: 0.2845
Epoch 6/128
 - 2s - loss: 1.3816 - accuracy: 0.2943 - val_loss: 1.3825 - val_accuracy: 0.2845
Epoch 7/128
 - 2s - loss: 1.3815 - accuracy: 0.2839 - val_loss: 1.3825 - val_accuracy: 0.2854
Epoch 8/128
 - 2s - loss: 1.3808 - accuracy: 0.2871 - val_loss: 1.3824 - val_accuracy: 0.2845
Epoch 9/128
 - 2s - loss: 1.3805 - accuracy: 0.2878 - val_loss: 1.3841 - val_accuracy: 0.2650
Epoch 10/128
 - 2s - loss: 1.3804 - accuracy: 0.2922 - val_loss: 1.3827 - val_accuracy: 0.2845
Epoch 11/128
 - 2s - loss: 1.3798 - accuracy: 0.2948 - val_loss: 1.3830 - val_accuracy: 0.2845
Epoch 12/128
 - 2s - loss: 1.3796 - accuracy: 0.2913 - val_loss: 1.3832 - val_accuracy: 0.2829
Epoch 13/128
 - 2s - loss: 1.3785 - accuracy: 0.2915 - val_loss: 1.3837 - val_accuracy: 0.2718
Epoch 14/128
 - 2s - loss: 1.3803 - accuracy: 0.2790 - val_loss: 1.3864 - val_accuracy: 0.2484
Epoch 15/128
 - 2s - loss: 1.3789 - accuracy: 0.2912 - val_loss: 1.3854 - val_accuracy: 0.2652
Epoch 16/128
 - 2s - loss: 1.3781 - accuracy: 0.2907 - val_loss: 1.3884 - val_accuracy: 0.2572
Epoch 17/128
 - 2s - loss: 1.3845 - accuracy: 0.2768 - val_loss: 1.3867 - val_accuracy: 0.2531
Epoch 18/128
 - 2s - loss: 1.3792 - accuracy: 0.2877 - val_loss: 1.3856 - val_accuracy: 0.2675
Epoch 19/128
 - 2s - loss: 1.3780 - accuracy: 0.2907 - val_loss: 1.3838 - val_accuracy: 0.2687
Epoch 20/128
 - 2s - loss: 1.3785 - accuracy: 0.2891 - val_loss: 1.3854 - val_accuracy: 0.2584
Epoch 21/128
 - 2s - loss: 1.3792 - accuracy: 0.2868 - val_loss: 1.3844 - val_accuracy: 0.2675
Epoch 22/128
 - 2s - loss: 1.3784 - accuracy: 0.2846 - val_loss: 1.3841 - val_accuracy: 0.2753
Epoch 23/128
 - 2s - loss: 1.3740 - accuracy: 0.3036 - val_loss: 1.3836 - val_accuracy: 0.2720
Epoch 24/128
 - 2s - loss: 1.3704 - accuracy: 0.3045 - val_loss: 1.3823 - val_accuracy: 0.2769
Epoch 25/128
 - 2s - loss: 1.3653 - accuracy: 0.3129 - val_loss: 1.3823 - val_accuracy: 0.2753
Epoch 26/128
 - 2s - loss: 1.3663 - accuracy: 0.3201 - val_loss: 1.3805 - val_accuracy: 0.2786
Epoch 27/128
 - 2s - loss: 1.3669 - accuracy: 0.3068 - val_loss: 1.3820 - val_accuracy: 0.2800
Epoch 28/128
 - 2s - loss: 1.3624 - accuracy: 0.3138 - val_loss: 1.3810 - val_accuracy: 0.2749
Epoch 29/128
 - 2s - loss: 1.3661 - accuracy: 0.3062 - val_loss: 1.3834 - val_accuracy: 0.2736
Epoch 30/128
 - 2s - loss: 1.3563 - accuracy: 0.3159 - val_loss: 1.3819 - val_accuracy: 0.2685
Epoch 31/128
 - 2s - loss: 1.3549 - accuracy: 0.3143 - val_loss: 1.3827 - val_accuracy: 0.2442
Epoch 32/128
 - 2s - loss: 1.3567 - accuracy: 0.3217 - val_loss: 1.3836 - val_accuracy: 0.2654
Epoch 33/128
 - 2s - loss: 1.3586 - accuracy: 0.3112 - val_loss: 1.3830 - val_accuracy: 0.2681
Epoch 34/128
 - 2s - loss: 1.3552 - accuracy: 0.3060 - val_loss: 1.3829 - val_accuracy: 0.2946
Epoch 35/128
 - 2s - loss: 1.3630 - accuracy: 0.3176 - val_loss: 1.4044 - val_accuracy: 0.2531
Epoch 36/128
 - 2s - loss: 1.3642 - accuracy: 0.2978 - val_loss: 1.3823 - val_accuracy: 0.3037
Epoch 37/128
 - 2s - loss: 1.3490 - accuracy: 0.3289 - val_loss: 1.3832 - val_accuracy: 0.3069
Epoch 38/128
 - 2s - loss: 1.3495 - accuracy: 0.3168 - val_loss: 1.3825 - val_accuracy: 0.2710
Epoch 39/128
 - 2s - loss: 1.3403 - accuracy: 0.3321 - val_loss: 1.3920 - val_accuracy: 0.2619
Epoch 40/128
 - 2s - loss: 1.3574 - accuracy: 0.3143 - val_loss: 1.3780 - val_accuracy: 0.3226
Epoch 41/128
 - 2s - loss: 1.3451 - accuracy: 0.3331 - val_loss: 1.3738 - val_accuracy: 0.3234
Epoch 42/128
 - 2s - loss: 1.3388 - accuracy: 0.3362 - val_loss: 1.3696 - val_accuracy: 0.3254
Epoch 43/128
 - 2s - loss: 1.3294 - accuracy: 0.3591 - val_loss: 1.3678 - val_accuracy: 0.3324
Epoch 44/128
 - 2s - loss: 1.3289 - accuracy: 0.3410 - val_loss: 1.3732 - val_accuracy: 0.3314
Epoch 45/128
 - 2s - loss: 1.3303 - accuracy: 0.3439 - val_loss: 1.3716 - val_accuracy: 0.3248
Epoch 46/128
 - 2s - loss: 1.3255 - accuracy: 0.3271 - val_loss: 1.3696 - val_accuracy: 0.3316
Epoch 47/128
 - 2s - loss: 1.3280 - accuracy: 0.3484 - val_loss: 1.3678 - val_accuracy: 0.3349
Epoch 48/128
 - 2s - loss: 1.3318 - accuracy: 0.3474 - val_loss: 1.3767 - val_accuracy: 0.3148
Epoch 49/128
 - 2s - loss: 1.3279 - accuracy: 0.3614 - val_loss: 1.3797 - val_accuracy: 0.3133
Epoch 50/128
 - 2s - loss: 1.3186 - accuracy: 0.3553 - val_loss: 1.3612 - val_accuracy: 0.3331
Epoch 51/128
 - 2s - loss: 1.3116 - accuracy: 0.3530 - val_loss: 1.3585 - val_accuracy: 0.3242
Epoch 52/128
 - 2s - loss: 1.3123 - accuracy: 0.3634 - val_loss: 1.3827 - val_accuracy: 0.3078
Epoch 53/128
 - 2s - loss: 1.3161 - accuracy: 0.3556 - val_loss: 1.3631 - val_accuracy: 0.3181
Epoch 54/128
 - 2s - loss: 1.3168 - accuracy: 0.3441 - val_loss: 1.3716 - val_accuracy: 0.3263
Epoch 55/128
 - 2s - loss: 1.3008 - accuracy: 0.3601 - val_loss: 1.3587 - val_accuracy: 0.3357
Epoch 56/128
 - 2s - loss: 1.3042 - accuracy: 0.3611 - val_loss: 1.3855 - val_accuracy: 0.2854
Epoch 57/128
 - 2s - loss: 1.3028 - accuracy: 0.3646 - val_loss: 1.3744 - val_accuracy: 0.3069
Epoch 58/128
 - 2s - loss: 1.3194 - accuracy: 0.3598 - val_loss: 1.3851 - val_accuracy: 0.2903
Epoch 59/128
 - 2s - loss: 1.3012 - accuracy: 0.3608 - val_loss: 1.3734 - val_accuracy: 0.2940
Epoch 60/128
 - 2s - loss: 1.2927 - accuracy: 0.3598 - val_loss: 1.3947 - val_accuracy: 0.2817
Epoch 61/128
 - 2s - loss: 1.2876 - accuracy: 0.3632 - val_loss: 1.3566 - val_accuracy: 0.3357
Epoch 62/128
 - 2s - loss: 1.2937 - accuracy: 0.3644 - val_loss: 1.3918 - val_accuracy: 0.3043
Epoch 63/128
 - 2s - loss: 1.2780 - accuracy: 0.3825 - val_loss: 1.3714 - val_accuracy: 0.3176
Epoch 64/128
 - 2s - loss: 1.2722 - accuracy: 0.3714 - val_loss: 1.3956 - val_accuracy: 0.2738
Epoch 65/128
 - 2s - loss: 1.2646 - accuracy: 0.3841 - val_loss: 1.4094 - val_accuracy: 0.2963
Epoch 66/128
 - 2s - loss: 1.2789 - accuracy: 0.3762 - val_loss: 1.4088 - val_accuracy: 0.3080
Epoch 67/128
 - 2s - loss: 1.2858 - accuracy: 0.3719 - val_loss: 1.3519 - val_accuracy: 0.3137
Epoch 68/128
 - 2s - loss: 1.2607 - accuracy: 0.3823 - val_loss: 1.3789 - val_accuracy: 0.3232
Epoch 69/128
 - 2s - loss: 1.2524 - accuracy: 0.3906 - val_loss: 1.3465 - val_accuracy: 0.3014
Epoch 70/128
 - 2s - loss: 1.2525 - accuracy: 0.3816 - val_loss: 1.3491 - val_accuracy: 0.3612
Epoch 71/128
 - 2s - loss: 1.2614 - accuracy: 0.3899 - val_loss: 1.3453 - val_accuracy: 0.3144
Epoch 72/128
 - 3s - loss: 1.2287 - accuracy: 0.3977 - val_loss: 1.3730 - val_accuracy: 0.3501
Epoch 73/128
 - 4s - loss: 1.2356 - accuracy: 0.4023 - val_loss: 1.3736 - val_accuracy: 0.3228
Epoch 74/128
 - 3s - loss: 1.2353 - accuracy: 0.3980 - val_loss: 1.3560 - val_accuracy: 0.3322
Epoch 75/128
 - 3s - loss: 1.2515 - accuracy: 0.3957 - val_loss: 1.3569 - val_accuracy: 0.3254
Epoch 76/128
 - 3s - loss: 1.2527 - accuracy: 0.3846 - val_loss: 1.4305 - val_accuracy: 0.2784
Epoch 77/128
 - 3s - loss: 1.2163 - accuracy: 0.4112 - val_loss: 1.3542 - val_accuracy: 0.3614
Epoch 78/128
 - 3s - loss: 1.1961 - accuracy: 0.4313 - val_loss: 1.3502 - val_accuracy: 0.3682
Epoch 79/128
 - 3s - loss: 1.1985 - accuracy: 0.4155 - val_loss: 1.3955 - val_accuracy: 0.3154
Epoch 80/128
 - 3s - loss: 1.2157 - accuracy: 0.4021 - val_loss: 1.3659 - val_accuracy: 0.3581
Epoch 81/128
 - 3s - loss: 1.2191 - accuracy: 0.4089 - val_loss: 1.3818 - val_accuracy: 0.3585
Epoch 82/128
 - 3s - loss: 1.2180 - accuracy: 0.4096 - val_loss: 1.3930 - val_accuracy: 0.3104
Epoch 83/128
 - 3s - loss: 1.2023 - accuracy: 0.4175 - val_loss: 1.4006 - val_accuracy: 0.3435
Epoch 84/128
 - 3s - loss: 1.1925 - accuracy: 0.4227 - val_loss: 1.3642 - val_accuracy: 0.3551
Epoch 85/128
 - 3s - loss: 1.1962 - accuracy: 0.4222 - val_loss: 1.3965 - val_accuracy: 0.3398
Epoch 86/128
 - 3s - loss: 1.1880 - accuracy: 0.4235 - val_loss: 1.3521 - val_accuracy: 0.3477
Epoch 87/128
 - 3s - loss: 1.1871 - accuracy: 0.4293 - val_loss: 1.3907 - val_accuracy: 0.3501
Epoch 88/128
 - 3s - loss: 1.1856 - accuracy: 0.4350 - val_loss: 1.3971 - val_accuracy: 0.3129
Epoch 89/128
 - 3s - loss: 1.1908 - accuracy: 0.4304 - val_loss: 1.4104 - val_accuracy: 0.3505
Epoch 90/128
 - 3s - loss: 1.1879 - accuracy: 0.4367 - val_loss: 1.3712 - val_accuracy: 0.3403
Epoch 91/128
 - 3s - loss: 1.1774 - accuracy: 0.4385 - val_loss: 1.3744 - val_accuracy: 0.3326
Epoch 92/128
 - 3s - loss: 1.1582 - accuracy: 0.4522 - val_loss: 1.5200 - val_accuracy: 0.3314
Epoch 93/128
 - 3s - loss: 1.1569 - accuracy: 0.4478 - val_loss: 1.4451 - val_accuracy: 0.3600
Epoch 94/128
 - 3s - loss: 1.1868 - accuracy: 0.4354 - val_loss: 1.4225 - val_accuracy: 0.2987
Epoch 95/128
 - 3s - loss: 1.1699 - accuracy: 0.4403 - val_loss: 1.4178 - val_accuracy: 0.3438
Epoch 96/128
 - 3s - loss: 1.1790 - accuracy: 0.4435 - val_loss: 1.3588 - val_accuracy: 0.3731
Epoch 97/128
 - 3s - loss: 1.1374 - accuracy: 0.4618 - val_loss: 1.4047 - val_accuracy: 0.3812
Epoch 98/128
 - 3s - loss: 1.1435 - accuracy: 0.4579 - val_loss: 1.4767 - val_accuracy: 0.3620
Epoch 99/128
 - 3s - loss: 1.1749 - accuracy: 0.4397 - val_loss: 1.4818 - val_accuracy: 0.3740
Epoch 100/128
 - 3s - loss: 1.1313 - accuracy: 0.4757 - val_loss: 1.5185 - val_accuracy: 0.3308
Epoch 101/128
 - 3s - loss: 1.1763 - accuracy: 0.4374 - val_loss: 1.4388 - val_accuracy: 0.3662
Epoch 102/128
 - 3s - loss: 1.1367 - accuracy: 0.4724 - val_loss: 1.3941 - val_accuracy: 0.3845
Epoch 103/128
 - 3s - loss: 1.1574 - accuracy: 0.4507 - val_loss: 1.3877 - val_accuracy: 0.3448
Epoch 104/128
 - 3s - loss: 1.1286 - accuracy: 0.4657 - val_loss: 1.4302 - val_accuracy: 0.3477
Epoch 105/128
 - 3s - loss: 1.1581 - accuracy: 0.4561 - val_loss: 1.4543 - val_accuracy: 0.3575
Epoch 106/128
 - 3s - loss: 1.1377 - accuracy: 0.4596 - val_loss: 1.4193 - val_accuracy: 0.3764
Epoch 107/128
 - 3s - loss: 1.1195 - accuracy: 0.4643 - val_loss: 1.4200 - val_accuracy: 0.3668
Epoch 108/128
 - 3s - loss: 1.1022 - accuracy: 0.4832 - val_loss: 1.4207 - val_accuracy: 0.3799
Epoch 109/128
 - 3s - loss: 1.1209 - accuracy: 0.4687 - val_loss: 1.4304 - val_accuracy: 0.3388
Epoch 110/128
 - 3s - loss: 1.1406 - accuracy: 0.4654 - val_loss: 1.3794 - val_accuracy: 0.3401
Epoch 111/128
 - 3s - loss: 1.1243 - accuracy: 0.4797 - val_loss: 1.4254 - val_accuracy: 0.3682
Epoch 112/128
 - 3s - loss: 1.0912 - accuracy: 0.4874 - val_loss: 1.4319 - val_accuracy: 0.3795
Epoch 113/128
 - 3s - loss: 1.0752 - accuracy: 0.5037 - val_loss: 1.3916 - val_accuracy: 0.3795
Epoch 114/128
 - 3s - loss: 1.1021 - accuracy: 0.4829 - val_loss: 1.4597 - val_accuracy: 0.3493
Epoch 115/128
 - 3s - loss: 1.1068 - accuracy: 0.4861 - val_loss: 1.4406 - val_accuracy: 0.3701
Epoch 116/128
 - 3s - loss: 1.0954 - accuracy: 0.4788 - val_loss: 1.4763 - val_accuracy: 0.3768
Epoch 117/128
 - 3s - loss: 1.0945 - accuracy: 0.4885 - val_loss: 1.4128 - val_accuracy: 0.3655
Epoch 118/128
 - 3s - loss: 1.0694 - accuracy: 0.5172 - val_loss: 1.5944 - val_accuracy: 0.3799
Epoch 119/128
 - 3s - loss: 1.1265 - accuracy: 0.4785 - val_loss: 1.4850 - val_accuracy: 0.3822
Epoch 120/128
 - 3s - loss: 1.0629 - accuracy: 0.5048 - val_loss: 1.4649 - val_accuracy: 0.3880
Epoch 121/128
 - 3s - loss: 1.0723 - accuracy: 0.4978 - val_loss: 1.5588 - val_accuracy: 0.3964
Epoch 122/128
 - 4s - loss: 1.0714 - accuracy: 0.5077 - val_loss: 1.4821 - val_accuracy: 0.3814
Epoch 123/128
 - 3s - loss: 1.0579 - accuracy: 0.5016 - val_loss: 1.4767 - val_accuracy: 0.3791
Epoch 124/128
 - 3s - loss: 1.0528 - accuracy: 0.5056 - val_loss: 1.7386 - val_accuracy: 0.4011
Epoch 125/128
 - 3s - loss: 1.0676 - accuracy: 0.5022 - val_loss: 1.5770 - val_accuracy: 0.3240
Epoch 126/128
 - 4s - loss: 1.0823 - accuracy: 0.5012 - val_loss: 1.4298 - val_accuracy: 0.3273
Epoch 127/128
 - 3s - loss: 1.0599 - accuracy: 0.5074 - val_loss: 1.4578 - val_accuracy: 0.3651
Epoch 128/128
 - 3s - loss: 1.0301 - accuracy: 0.5410 - val_loss: 1.3976 - val_accuracy: 0.3986

Fit: epochs= 128 , batch_size= 32 , verbose= 2 , shuffle= False , validation_split= 0.2 

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_2 (Dense)              (None, 500)               5500      
_________________________________________________________________
dense_3 (Dense)              (None, 300)               150300    
_________________________________________________________________
dense_4 (Dense)              (None, 200)               60200     
_________________________________________________________________
dense_5 (Dense)              (None, 100)               20100     
_________________________________________________________________
dense_6 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_7 (Dense)              (None, 20)                1020      
_________________________________________________________________
dense_8 (Dense)              (None, 4)                 84        
=================================================================
Total params: 242,364
Trainable params: 242,364
Non-trainable params: 0
_________________________________________________________________
None

Accuracy Train: 49.65%
Accuracy Test: 25.65%
Loss Train: 1.15
Loss Test: 6.53
Numero dati esaminati: 4280
True Positive 1098
False Positive 3182
